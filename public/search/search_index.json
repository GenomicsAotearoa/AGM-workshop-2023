{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the AGM training day 2023 \u00b6 Reproducibility with Nextflow and nf-core \u00b6 During this workshop we will establish the fundamental ideas and skills that are required for using Nextflow as a workflow manager. We will access an existing pipeline from the free repository nf-core and run this pipeline on a test dataset as an example. The workshop is organised into three sections: \u00b6 In session one we will provide an overview of Nextflow, introducing terminology, the format, and commands for working in the Nextflow environment. In session two we will introduce nf-core - a repository of existing pipelines that can be easily accessed to quick-start an analysis. We will cover the general structure of a pipeline, and then use an example pipeline to familiarise you with running and customizing a workflow. In the third and final session we will focus on that idea of customizing and configuring pipelines, as well as the metrics and shareable reports that you might generate during a workflow. Generally speaking, this workshop is designed only as an introduction to Nextflow and aims to provide a persuasive argument for the use of a workflow manager. This workshop cannot be exhaustive, so at the end of the workshop I propose the creation of Nextflow group either on slack or github. Finally, we will end the workshop with a short, standalone session on github use.","title":"Home"},{"location":"#welcome-to-the-agm-training-day-2023","text":"","title":"Welcome to the AGM training day 2023"},{"location":"#reproducibility-with-nextflow-and-nf-core","text":"During this workshop we will establish the fundamental ideas and skills that are required for using Nextflow as a workflow manager. We will access an existing pipeline from the free repository nf-core and run this pipeline on a test dataset as an example.","title":"Reproducibility with Nextflow and nf-core"},{"location":"#the-workshop-is-organised-into-three-sections","text":"In session one we will provide an overview of Nextflow, introducing terminology, the format, and commands for working in the Nextflow environment. In session two we will introduce nf-core - a repository of existing pipelines that can be easily accessed to quick-start an analysis. We will cover the general structure of a pipeline, and then use an example pipeline to familiarise you with running and customizing a workflow. In the third and final session we will focus on that idea of customizing and configuring pipelines, as well as the metrics and shareable reports that you might generate during a workflow. Generally speaking, this workshop is designed only as an introduction to Nextflow and aims to provide a persuasive argument for the use of a workflow manager. This workshop cannot be exhaustive, so at the end of the workshop I propose the creation of Nextflow group either on slack or github. Finally, we will end the workshop with a short, standalone session on github use.","title":"The workshop is organised into three sections:"},{"location":"session_1/0_kickoff/","text":"Setup your environment \u00b6 It is good practice to organise projects into their own folders to make it easier to track and replicate experiments over time. Start by creating a new directory for all of today\u2019s activities and move into it: mkdir ~/session1 && cd $_","title":"Setup your environment"},{"location":"session_1/0_kickoff/#setup-your-environment","text":"It is good practice to organise projects into their own folders to make it easier to track and replicate experiments over time. Start by creating a new directory for all of today\u2019s activities and move into it: mkdir ~/session1 && cd $_","title":"Setup your environment"},{"location":"session_1/1_introToNextflow/","text":"Introduction to Nextflow \u00b6 Objectives Learn about the core features of Nextflow Learn Nextflow terminology Learn fundamental commands and options for executing pipelines What is Nextflow? \u00b6 Nextflow is a workflow orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational pipelines. It is designed around the idea that the Linux platform is the lingua franca of data science. Linux provides many simple but powerful command-line and scripting tools that, when chained together, facilitate complex data manipulations. Nextflow extends this approach, adding the ability to define complex program interactions and a high-level parallel computational environment based on the dataflow programming model. Nextflow\u2019s core features are: Pipeline portability and reproducibility Scalability of parallelization and deployment Integration of existing tools, systems, and industry standards Whether you are working with genomics data or other large and complex data sets, Nextflow can help you to streamline your pipeline and improve your productivity. Processes and Channels \u00b6 In Nextflow, processes and channels are the fundamental building blocks of a pipeline. A process is a unit of execution that represents a single computational step in a pipeline. It is defined as a block of code that typically performs one specific task. Each process will specify its input and outputs, as well as any directives and conditional statements required for its execution. Processes can be written in any language that can be executed from the command line, such as Bash, Python, Perl, or R. Processes in are executed independently (i.e., they do not share a common writable state) as tasks . Multiple tasks can run in parallel, allowing for efficient utilization of computing resources. Nextflow is a top down pipeline manager and will automatically manage data dependencies between processes, ensuring that each process is executed only when its input data is available and all of its dependencies have been satisfied. A channel is an asynchronous first-in, first-out (FIFO) queue that is used to join processes together. Channels allow data to passed between processes and can be used to manage data, parallelize tasks, and structure pipelines. Any process can define one or more channels as an input and output. Ultimately the pipeline execution flow itself, is implicitly defined by the channel declarations. Importantly, processes can be parameterised to allow for flexibility in their behavior and to enable their reuse in and between pipelines. Parameters can be defined in the process declaration and can be passed to the process at runtime. Parameters can be used to specify the input and output files, as well as any other parameters required for the process to execute. Execution abstraction \u00b6 While a process defines what command or script is executed, the executor determines how and where the script is executed. Nextflow provides an abstraction between the pipeline\u2019s functional logic and the underlying execution system. This abstraction allows users to define a pipeline once and execute it on different computing platforms without having to modify the pipeline definition. If not specified, Nextflow will execute locally. Executing locally is useful for pipeline development and testing purposes. However, for real-world computational pipelines, a high-performance computing (HPC) or cloud platform is often required. Nextflow provides a variety of built-in execution options, such as local execution, HPC cluster execution, and cloud-based execution, and allows users to easily switch between these options using command-line arguments. You can find a full list of supported executors as well as how to configure them in the Nextflow docs . Nextflow CLI \u00b6 Nextflow implements a declarative domain-specific language (DSL) that simplifies the writing of complex data analysis pipelines as an extension of a general-purpose programming language. As a concise DSL, Nextflow handles recurrent use cases while having the flexibility and power to handle corner cases. Nextflow is an extension of the Groovy programming language which, in turn, is a super-set of the Java programming language. Groovy can be thought of as \u201cPython for Java\u201d and simplifies the code. Nextflow provides a robust command line interface for the management and execution of pipelines. Nextflow can be used on any POSIX compatible system (Linux, OS X, etc). It requires Bash 3.2 (or later) and Java 11 (or later) to be installed. Nextflow is distributed as a self-installing package and does not require any special installation procedure. How to install Nextflow locally Download the executable package using either wget -qO- https://get.nextflow.io | bash or curl -s https://get.nextflow.io | bash Make the binary executable on your system by running chmod +x nextflow Move the nextflow file to a directory accessible by your $PATH variable, e.g, mv nextflow ~/bin/ Nextflow options and commands \u00b6 Nextflow provides a robust command line interface for the management and execution of pipelines. The top-level interface consists of options and commands. You can list Nextflow options and commands with the -h option: nextflow -h Output Usage: nextflow [options] COMMAND [arg...] Options: -C Use the specified configuration file(s) overriding any defaults -D Set JVM properties -bg Execute nextflow in background -c, -config Add the specified file to configuration set -config-ignore-includes Disable the parsing of config includes -d, -dockerize Launch nextflow via Docker (experimental) [truncated] Commands: clean Clean up project cache and work directories clone Clone a project into a folder config Print a project configuration [truncated] Options for a commands can also be viewed by appending the -help option to a Nextflow command. For example, options for the the run command can be viewed: nextflow run -help Output Execute a pipeline project Usage: run [options] Project name or repository url Options: -E Exports all current system environment Default: false -ansi-log Enable/disable ANSI console logging -bucket-dir Remote bucket where intermediate result files are stored -cache Enable/disable processes caching -disable-jobs-cancellation Prevent the cancellation of child jobs on execution termination -dsl1 Execute the workflow using DSL1 syntax Default: false -dsl2 Execute the workflow using DSL2 syntax Default: false -dump-channels Dump channels for debugging purpose -dump-hashes Dump task hash keys for debugging purpose Default: false [truncated] Exercise Find out which version of Nextflow you are using. Solution You can find out which version of Nextflow you are using by executing: nextflow -version You should see the following: Output N E X T F L O W version 23.04.4 build 5881 created 25-09-2023 15:34 UTC (26-09-2023 04:34 NZDT) cite doi:10.1038/nbt.3820 http://nextflow.io Managing your environment \u00b6 You can use environment variables to control the Nextflow runtime and the underlying Java virtual machine. These variables can be exported before running a pipeline and will be interpreted by Nextflow. For most users, Nextflow will work without setting any environment variables. However, to improve reproducibility and to optimise your resources, you will benefit from establishing some of these. For example, for consistency, it is good practice to pin the version of Nextflow you are using with the NXF_VER variable: export NXF_VER = <version number> Exercise Change the version of Nextflow you are using to 23.04.0 by exporting an environmental variable: Solution Export the singularity cache using the NXF_VER environmental variable: export NXF_VER = 23 .04.0 Check that the NXF_VER has been applied: nextflow -version You should see nextflow update and print the following: Output N E X T F L O W version 23.04.0 build 5857 created 01-04-2023 21:09 UTC (02-04-2023 09:09 NZDT) cite doi:10.1038/nbt.3820 http://nextflow.io Environmental variables on NeSI The behaviour of Nextflow environmental variables won't work as expected if using a NeSI Nextflow module. Similarly, if you are using a shared resource, you may also consider including paths to where software is stored and can be accessed using the NXF_SINGULARITY_CACHEDIR or the NXF_CONDA_CACHEDIR variables: export NXF_SINGULARITY_CACHEDIR = <custom/path/to/conda/cache> Exercise Export the folder /nesi/nobackup/nesi02659/nextflow-workshop as the folder where remote Singularity images are stored: Solution Export the singularity cache using the NXF_SINGULARITY_CACHEDIR environmental variable: export NXF_SINGULARITY_CACHEDIR = /nesi/nobackup/nesi02659/nextflow-workshop Check that the NXF_SINGULARITY_CACHEDIR has been exported: echo $NXF_SINGULARITY_CACHEDIR How to manage environmental variables You may want to include these, or other environmental variables, in your .bashrc file (or alternate) that is loaded when you log in so you don\u2019t need to export variables every session. A complete list of environmental variables can be found in the Nextflow docs . Executing a pipeline \u00b6 Nextflow seamlessly integrates with code repositories such as GitHub . This feature allows you to manage your project code and use public Nextflow pipelines quickly, consistently, and transparently. The Nextflow pull command will download a pipeline from a hosting platform into your global cache $HOME/.nextflow/assets folder. If you are pulling a project hosted in a remote code repository, you can specify its qualified name or the repository URL. The qualified name is formed by two parts - the owner name and the repository name separated by a / character. For example, if a Nextflow project bar is hosted in a GitHub repository foo at the address http://github.com/foo/bar , it could be pulled using: nextflow pull foo/bar Or by using the complete URL: nextflow pull http://github.com/foo/bar Alternatively, the Nextflow clone command can be used to download a pipeline into a local directory of your choice: nextflow clone foo/bar <your/path> The Nextflow run command is used to initiate the execution of a pipeline: nextflow run foo/bar If you run a pipeline, it will look for a local file with the pipeline name you\u2019ve specified. If that file does not exist, it will look for a public repository with the same name on GitHub (unless otherwise specified). If it is found, Nextflow will automatically pull the pipeline to your global cache and execute it. Warning Be aware of what is already in your current working directory where you launch your pipeline. If your current working directory contains nextflow configuration files you may encounter unexpected results. Exercise Execute the hello pipeline directly from nextflow-io GitHub repository. Solution Use the run command to execute the nextflow-io/hello pipeline: nextflow run nextflow-io/hello Output N E X T F L O W ~ version 23.04.0 Pulling nextflow-io/hello ... downloaded from https://github.com/nextflow-io/hello.git Launching `https://github.com/nextflow-io/hello` [silly_sax] DSL2 - revision: 1d71f857bb [master] executor > local (4) [e6/2132d2] process > sayHello (3) [100%] 4 of 4 \u2714 Hola world! Bonjour world! Ciao world! Hello world! More information about the Nextflow run command can be found in the Nextflow docs . Executing a revision \u00b6 When a Nextflow pipeline is created or updated using GitHub (or another code repository), a new revision is created. Each revision is identified by a unique number, which can be used to track changes made to the pipeline and to ensure that the same version of the pipeline is used consistently across different runs. The Nextflow info command can be used to view pipeline properties, such as the project name, repository, local path, main script, and revisions. The * indicates which revision of the pipeline you have stickied and will be executed when using the run command. nextflow info <pipeline> It is recommended that you use the revision flag every time you execute a pipeline to ensure that the version is correct. To use a specific revision, you simply need to add it to the command line with the --revision or -r flag. For example, to run a pipeline with the v1.0 revision, you would use the following: nextflow run <pipeline> -r v1.0 Nextflow automatically provides built-in support for version control using Git. With this, users can easily manage and track changes made to a pipeline over time. A revision can be a git branch , tag or commit SHA number, and can be used interchangeably. Exercise Execute the hello pipeline directly from the nextflow-io GitHub using the v1.1 revision tag. Solution Use the nextflow run command to execute the nextflow-io/hello pipeline with the v1.1 revision tag: nextflow run nextflow-io/hello -r v1.1 Output N E X T F L O W ~ version 23.04.0 NOTE: Your local project version looks outdated - a different revision is available in the remote repository [3b355db864] Nextflow DSL1 is no longer supported \u2014 Update your script to DSL2, or use Nextflow 22.10.x or earlier Warning The warning shown above is expected as the v1.1 pipeline revision was written using an older version of Nextflow that uses the depreciated echo method. As both Nextflow and pipelines are updated independently over time, pipelines and Nextflow functions can get out of sync. While most nf-core pipelines are now dsl2 (the current way of writing pipelines), some are still written in dsl1 and may require older version of Nextflow. You can use an older version of nextflow on the fly by adding adding the environmental variable to the start of the run command NXF_VER = 22 .10.0 nextflow run nextflow-io/hello -r v1.1 Output N E X T F L O W ~ version 22.10.0 NOTE: Your local project version looks outdated - a different revision is available in the remote repository [3b355db864] Launching `https://github.com/nextflow-io/hello` [amazing_lovelace] DSL1 - revision: baba3959d7 [v1.1] WARN: The use of `echo` method has been deprecated executor > local (4) [e6/cfda06] process > sayHello (4) [100%] 4 of 4 \u2714 Bojour world! (version 1.1) Hello world! (version 1.1) Ciao world! (version 1.1) Hola world! (version 1.1) Nextflow log \u00b6 It is important to keep a record of the commands you have run to generate your results. Nextflow helps with this by creating and storing metadata and logs about the run in hidden files and folders in your current directory (unless otherwise specified). This data can be used by Nextflow to generate reports. It can also be queried using the Nextflow log command: nextflow log The log command has multiple options to facilitate the queries and is especially useful while debugging a pipeline and inspecting execution metadata. You can view all of the possible log options with -h flag: nextflow log -h To query a specific execution you can use the RUN NAME or a SESSION ID : nextflow log <run name> To get more information, you can use the -f option with named fields. For example: nextflow log <run name> -f process,hash,duration There are many other fields you can query. You can view a full list of fields with the -l option: nextflow log -l Exercise Use the log command to view with process , hash , and script fields for your tasks from your most recent Nextflow execution. Solution Use the log command to get a list of you recent executions: nextflow log Output TIMESTAMP DURATION RUN NAME STATUS REVISION ID SESSION ID COMMAND 2023-08-29 07:33:48 3.6s stupefied_bernard OK 1d71f857bb f9e18b71-d689-4589-be34-8cd98c1aab2e nextflow run nextflow-io/hello Query the process, hash, and script using the -f option for the most recent run: nextflow log stupefied_bernard -f process,hash,script Output sayHello f3/8f827f echo 'Hola world!' sayHello b8/b66545 echo 'Ciao world!' sayHello 3c/498a68 echo 'Bonjour world!' sayHello 6e/8d5b1a echo 'Hello world!' Execution cache and resume \u00b6 Task execution caching is an essential feature of modern pipeline managers. Accordingly, Nextflow provides an automated caching mechanism for every execution. When using the Nextflow -resume option, successfully completed tasks from previous executions are skipped and the previously cached results are used in downstream tasks. Nextflow caching mechanism works by assigning a unique ID to each task. The task unique ID is generated as a 128-bit hash value composing the the complete file path, file size, and last modified timestamp. These ID's are used to create a separate execution directory where the tasks are executed and the outputs are stored. Nextflow will take care of the inputs and outputs in these folders for you. A multi-step pipeline is required to demonstrate cache and resume. The christopher-hakkaart/nf-core-demo pipeline was created with the nf-core create command and has the same structure as nf-core pipelines. It is a toy example with 3 processes: SAMPLESHEET_CHECK Executes a custom python script to check the input sample sheet is valid. FASTQC Executes FastQC using the .fastq.gz files from the sample sheet as inputs. MULTIQC Executes MultiQC using the FastQC reports generated by the FASTQC process. The christopher-hakkaartnf-core-demo is a very small nf-core pipeline. It uses real data and bioinformatics software and requires additional configuration to run successfully. To run this example you will need to include two profiles in your execution command. Profiles are sets of configuration options that can be accessed by Nextflow. Profiles will be explained in greater detail during the configuring nf-core pipelines section of the workshop. To run this pipeline, both the test profile and a software management profile (such as singularity ) are required: nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main This run requires 6GB of memory If your session was spawned with less than 8GB of memory, above run will fail with the following error Caused by: Process requirement exceeds available memory -- req: 6 GB; avail: 4 GB The command line output will print something like this: Output N E X T F L O W ~ version 23.04.0 Launching `https://github.com/christopher-hakkaart/nf-core-demo` [voluminous_kay] DSL2 - revision: 17521af3a8 [main] ------------------------------------------------------ ,--./,-. ___ __ __ __ ___ /,-._.--~' |\\ | |__ __ / ` / \\ |__) |__ } { | \\| | \\__, \\__/ | \\ |___ \\`-._,-`-, `._,._,' nf-core/demo v1.0dev-g17521af ------------------------------------------------------ Core Nextflow options revision : main runName : voluminous_kay containerEngine : singularity launchDir : /scale_wlg_persistent/filesets/home/chrishakk/session1 workDir : /scale_wlg_persistent/filesets/home/chrishakk/session1/work projectDir : /home/chrishakk/.nextflow/assets/christopher-hakkaart/nf-core-demo userName : chrishakk profile : test,singularity configFiles : /home/chrishakk/.nextflow/assets/christopher-hakkaart/nf-core-demo/nextflow.config Input/output options input : https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/samplesheet/samplesheet_test_illumina_amplicon.csv outdir : results Reference genome options genome : R64-1-1 fasta : s3://ngi-igenomes/igenomes/Saccharomyces_cerevisiae/Ensembl/R64-1-1/Sequence/WholeGenomeFasta/genome.fa Institutional config options config_profile_name : Test profile config_profile_description: Minimal test dataset to check pipeline function Max job request options max_cpus : 2 max_memory : 6.GB max_time : 6.h Generic options tracedir : null/pipeline_info !! Only displaying parameters that differ from the pipeline defaults !! ------------------------------------------------------ If you use nf-core/demo for your analysis please cite: * The nf-core framework https://doi.org/10.1038/s41587-020-0439-x * Software dependencies https://github.com/nf-core/demo/blob/master/CITATIONS.md ------------------------------------------------------ Downloading plugin nf-amazon@1.16.1 [f2/e5eb26] process > NFCORE_DEMO:DEMO:INPUT_CHECK:SAMPLESHEET_CHECK (samplesheet_test_illumina_amplicon.csv) [100%] 1 of 1 \u2714 [bb/f98425] process > NFCORE_DEMO:DEMO:FASTQC (SAMPLE1_PE_T1) [100%] 4 of 4 \u2714 [dd/728742] process > NFCORE_DEMO:DEMO:MULTIQC [100%] 1 of 1 \u2714 - Completed at: 29-Sep-2023 22:16:49 Duration : 2m 27s CPU hours : (a few seconds) Succeeded : 6 Executing this pipeline will create a work directory and a results directory with selected results files. In the output above, the hexadecimal numbers, such as bb/f98425 , identify the unique task execution. These numbers are also the prefix of the work directories where each task is executed. You can inspect the files produced by a task by looking inside the work directory and using these numbers to find the task-specific execution path: The files that have been selected for publication in the results folder can also be explored: ls results If you look inside the work directory of a FASTQC task, you will find the files that were staged and created when this task was executed: The FASTQC process runs four times, executing in a different work directories for each set of inputs. Therefore, in the previous example, the work directory [bb/f98425] represents just one of the four sets of input data that was processed. To print all the relevant paths to the screen, use the -ansi-log option can be used when executing your pipeline: nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main -ansi-log false It's very likely you will execute a pipeline multiple times as you find the parameters that best suit your data. You can save a lot of spaces (and time) if you resume a pipeline from the last step that was completed successfully or unmodified. By adding the -resume option to your run command you can use the cache rather than re-running successful tasks: nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main -resume If you run the christopher-hakkaart/nf-core-deme pipeline again without making any changes you will see that the cache is used: Output [truncated] [5f/07e477] process > NFCORE_DEMO:DEMO:INPUT_CHECK:SAMPLESHEET_CHECK (samplesheet_test_illumina_amplicon.csv) [100%] 1 of 1, cached: 1 \u2714 [b2/873706] process > NFCORE_DEMO:DEMO:FASTQC (SAMPLE2_PE_T1) [100%] 4 of 4, cached: 4 \u2714 [ca/e8e0a8] process > NFCORE_DEMO:DEMO:MULTIQC [100%] 1 of 1, cached: 1 \u2714 [truncated] In practical terms, the pipeline is executed from the beginning. However, before launching the execution of a process, Nextflow uses the task unique ID to check if the work directory already exists and that it contains a valid command exit state with the expected output files. If this condition is satisfied, the task execution is skipped and previously computed results are used as the process results. Notably, the -resume functionality is very sensitive. Even touching a file in the work directory can invalidate the cache. Exercise Invalidate the cache by touching a .fastq.gz file in a FASTQC task work directory (you can use the touch command). Execute the pipeline again with the -resume option to show that the cache has been invalidated. Solution Execute the pipeline for the first time (if you have not already). nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main Use the task ID shown for the FASTQC process and use it to find and touch the sample1_R1.fastq.gz file: touch work/b2/87370687cc7cdec037ce4f36807d32/sample1_R1.fastq.gz Execute the pipeline again with the -resume command option: nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main -resume You should that 2 of 4 tasks for FASTQC and the MULTIQC task were invalid and were executed again. Why did this happen? In this example, the cache of two FASTQC tasks were invalid. The sample1_R1.fastq.gz file is used by in the samplesheet twice. Thus, touching the symlink for this file and changing the date of last modification disrupted two task executions. Your work directory can get very big very quickly (especially if you are using full sized datasets). It is good practise to clean your work directory regularly. Rather than removing the work folder with all of it's contents, the Nextflow clean function allows you to selectively remove data associated with specific runs. nextflow clean -help The -after , -before , and -but options are all very useful to select specific runs to clean . The -dry-run option is also very useful to see which files will be removed if you were to -force the clean command. Exercise You Nextflow to clean your work work directory of staged files but keep your execution logs. Solution Use the Nextflow clean command with the -k and -f options: nextflow clean -k -f Listing and dropping cached pipelines \u00b6 Over time, you might want to remove a stored pipelines. Nextflow also has functionality to help you to view and remove pipelines that have been pulled locally. The Nextflow list command prints the projects stored in your global cache folder ( $HOME/.nextflow/assets ). These are the pipelines that were pulled when you executed either of the Nextflow pull or run commands: nextflow list If you want to remove a pipeline from your cache you can remove it using the Nextflow drop command: nextflow drop < pipeline> Exercise View your cached pipelines with the Nextflow list command and remove the nextflow-io/hello pipeline with the drop command. Solution List your pipeline assets: nextflow list Drop the nextflow-io/hello pipeline: nextflow drop nextflow-io/hello Check it has been removed: nextflow list Key points Nextflow is a pipeline orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational pipelines Environment variables can be used to control your Nextflow runtime and the underlying Java virtual machine Nextflow supports version control and has automatic integrations with online code repositories. Nextflow will cache your runs and they can be resumed with the -resume option You can manage pipelines with Nextflow commands (e.g., pull , clone , list , and drop )","title":"Introduction to Nextflow"},{"location":"session_1/1_introToNextflow/#introduction-to-nextflow","text":"Objectives Learn about the core features of Nextflow Learn Nextflow terminology Learn fundamental commands and options for executing pipelines","title":"Introduction to Nextflow"},{"location":"session_1/1_introToNextflow/#what-is-nextflow","text":"Nextflow is a workflow orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational pipelines. It is designed around the idea that the Linux platform is the lingua franca of data science. Linux provides many simple but powerful command-line and scripting tools that, when chained together, facilitate complex data manipulations. Nextflow extends this approach, adding the ability to define complex program interactions and a high-level parallel computational environment based on the dataflow programming model. Nextflow\u2019s core features are: Pipeline portability and reproducibility Scalability of parallelization and deployment Integration of existing tools, systems, and industry standards Whether you are working with genomics data or other large and complex data sets, Nextflow can help you to streamline your pipeline and improve your productivity.","title":"What is Nextflow?"},{"location":"session_1/1_introToNextflow/#processes-and-channels","text":"In Nextflow, processes and channels are the fundamental building blocks of a pipeline. A process is a unit of execution that represents a single computational step in a pipeline. It is defined as a block of code that typically performs one specific task. Each process will specify its input and outputs, as well as any directives and conditional statements required for its execution. Processes can be written in any language that can be executed from the command line, such as Bash, Python, Perl, or R. Processes in are executed independently (i.e., they do not share a common writable state) as tasks . Multiple tasks can run in parallel, allowing for efficient utilization of computing resources. Nextflow is a top down pipeline manager and will automatically manage data dependencies between processes, ensuring that each process is executed only when its input data is available and all of its dependencies have been satisfied. A channel is an asynchronous first-in, first-out (FIFO) queue that is used to join processes together. Channels allow data to passed between processes and can be used to manage data, parallelize tasks, and structure pipelines. Any process can define one or more channels as an input and output. Ultimately the pipeline execution flow itself, is implicitly defined by the channel declarations. Importantly, processes can be parameterised to allow for flexibility in their behavior and to enable their reuse in and between pipelines. Parameters can be defined in the process declaration and can be passed to the process at runtime. Parameters can be used to specify the input and output files, as well as any other parameters required for the process to execute.","title":"Processes and Channels"},{"location":"session_1/1_introToNextflow/#execution-abstraction","text":"While a process defines what command or script is executed, the executor determines how and where the script is executed. Nextflow provides an abstraction between the pipeline\u2019s functional logic and the underlying execution system. This abstraction allows users to define a pipeline once and execute it on different computing platforms without having to modify the pipeline definition. If not specified, Nextflow will execute locally. Executing locally is useful for pipeline development and testing purposes. However, for real-world computational pipelines, a high-performance computing (HPC) or cloud platform is often required. Nextflow provides a variety of built-in execution options, such as local execution, HPC cluster execution, and cloud-based execution, and allows users to easily switch between these options using command-line arguments. You can find a full list of supported executors as well as how to configure them in the Nextflow docs .","title":"Execution abstraction"},{"location":"session_1/1_introToNextflow/#nextflow-cli","text":"Nextflow implements a declarative domain-specific language (DSL) that simplifies the writing of complex data analysis pipelines as an extension of a general-purpose programming language. As a concise DSL, Nextflow handles recurrent use cases while having the flexibility and power to handle corner cases. Nextflow is an extension of the Groovy programming language which, in turn, is a super-set of the Java programming language. Groovy can be thought of as \u201cPython for Java\u201d and simplifies the code. Nextflow provides a robust command line interface for the management and execution of pipelines. Nextflow can be used on any POSIX compatible system (Linux, OS X, etc). It requires Bash 3.2 (or later) and Java 11 (or later) to be installed. Nextflow is distributed as a self-installing package and does not require any special installation procedure. How to install Nextflow locally Download the executable package using either wget -qO- https://get.nextflow.io | bash or curl -s https://get.nextflow.io | bash Make the binary executable on your system by running chmod +x nextflow Move the nextflow file to a directory accessible by your $PATH variable, e.g, mv nextflow ~/bin/","title":"Nextflow CLI"},{"location":"session_1/1_introToNextflow/#nextflow-options-and-commands","text":"Nextflow provides a robust command line interface for the management and execution of pipelines. The top-level interface consists of options and commands. You can list Nextflow options and commands with the -h option: nextflow -h Output Usage: nextflow [options] COMMAND [arg...] Options: -C Use the specified configuration file(s) overriding any defaults -D Set JVM properties -bg Execute nextflow in background -c, -config Add the specified file to configuration set -config-ignore-includes Disable the parsing of config includes -d, -dockerize Launch nextflow via Docker (experimental) [truncated] Commands: clean Clean up project cache and work directories clone Clone a project into a folder config Print a project configuration [truncated] Options for a commands can also be viewed by appending the -help option to a Nextflow command. For example, options for the the run command can be viewed: nextflow run -help Output Execute a pipeline project Usage: run [options] Project name or repository url Options: -E Exports all current system environment Default: false -ansi-log Enable/disable ANSI console logging -bucket-dir Remote bucket where intermediate result files are stored -cache Enable/disable processes caching -disable-jobs-cancellation Prevent the cancellation of child jobs on execution termination -dsl1 Execute the workflow using DSL1 syntax Default: false -dsl2 Execute the workflow using DSL2 syntax Default: false -dump-channels Dump channels for debugging purpose -dump-hashes Dump task hash keys for debugging purpose Default: false [truncated] Exercise Find out which version of Nextflow you are using. Solution You can find out which version of Nextflow you are using by executing: nextflow -version You should see the following: Output N E X T F L O W version 23.04.4 build 5881 created 25-09-2023 15:34 UTC (26-09-2023 04:34 NZDT) cite doi:10.1038/nbt.3820 http://nextflow.io","title":"Nextflow options and commands"},{"location":"session_1/1_introToNextflow/#managing-your-environment","text":"You can use environment variables to control the Nextflow runtime and the underlying Java virtual machine. These variables can be exported before running a pipeline and will be interpreted by Nextflow. For most users, Nextflow will work without setting any environment variables. However, to improve reproducibility and to optimise your resources, you will benefit from establishing some of these. For example, for consistency, it is good practice to pin the version of Nextflow you are using with the NXF_VER variable: export NXF_VER = <version number> Exercise Change the version of Nextflow you are using to 23.04.0 by exporting an environmental variable: Solution Export the singularity cache using the NXF_VER environmental variable: export NXF_VER = 23 .04.0 Check that the NXF_VER has been applied: nextflow -version You should see nextflow update and print the following: Output N E X T F L O W version 23.04.0 build 5857 created 01-04-2023 21:09 UTC (02-04-2023 09:09 NZDT) cite doi:10.1038/nbt.3820 http://nextflow.io Environmental variables on NeSI The behaviour of Nextflow environmental variables won't work as expected if using a NeSI Nextflow module. Similarly, if you are using a shared resource, you may also consider including paths to where software is stored and can be accessed using the NXF_SINGULARITY_CACHEDIR or the NXF_CONDA_CACHEDIR variables: export NXF_SINGULARITY_CACHEDIR = <custom/path/to/conda/cache> Exercise Export the folder /nesi/nobackup/nesi02659/nextflow-workshop as the folder where remote Singularity images are stored: Solution Export the singularity cache using the NXF_SINGULARITY_CACHEDIR environmental variable: export NXF_SINGULARITY_CACHEDIR = /nesi/nobackup/nesi02659/nextflow-workshop Check that the NXF_SINGULARITY_CACHEDIR has been exported: echo $NXF_SINGULARITY_CACHEDIR How to manage environmental variables You may want to include these, or other environmental variables, in your .bashrc file (or alternate) that is loaded when you log in so you don\u2019t need to export variables every session. A complete list of environmental variables can be found in the Nextflow docs .","title":"Managing your environment"},{"location":"session_1/1_introToNextflow/#executing-a-pipeline","text":"Nextflow seamlessly integrates with code repositories such as GitHub . This feature allows you to manage your project code and use public Nextflow pipelines quickly, consistently, and transparently. The Nextflow pull command will download a pipeline from a hosting platform into your global cache $HOME/.nextflow/assets folder. If you are pulling a project hosted in a remote code repository, you can specify its qualified name or the repository URL. The qualified name is formed by two parts - the owner name and the repository name separated by a / character. For example, if a Nextflow project bar is hosted in a GitHub repository foo at the address http://github.com/foo/bar , it could be pulled using: nextflow pull foo/bar Or by using the complete URL: nextflow pull http://github.com/foo/bar Alternatively, the Nextflow clone command can be used to download a pipeline into a local directory of your choice: nextflow clone foo/bar <your/path> The Nextflow run command is used to initiate the execution of a pipeline: nextflow run foo/bar If you run a pipeline, it will look for a local file with the pipeline name you\u2019ve specified. If that file does not exist, it will look for a public repository with the same name on GitHub (unless otherwise specified). If it is found, Nextflow will automatically pull the pipeline to your global cache and execute it. Warning Be aware of what is already in your current working directory where you launch your pipeline. If your current working directory contains nextflow configuration files you may encounter unexpected results. Exercise Execute the hello pipeline directly from nextflow-io GitHub repository. Solution Use the run command to execute the nextflow-io/hello pipeline: nextflow run nextflow-io/hello Output N E X T F L O W ~ version 23.04.0 Pulling nextflow-io/hello ... downloaded from https://github.com/nextflow-io/hello.git Launching `https://github.com/nextflow-io/hello` [silly_sax] DSL2 - revision: 1d71f857bb [master] executor > local (4) [e6/2132d2] process > sayHello (3) [100%] 4 of 4 \u2714 Hola world! Bonjour world! Ciao world! Hello world! More information about the Nextflow run command can be found in the Nextflow docs .","title":"Executing a pipeline"},{"location":"session_1/1_introToNextflow/#executing-a-revision","text":"When a Nextflow pipeline is created or updated using GitHub (or another code repository), a new revision is created. Each revision is identified by a unique number, which can be used to track changes made to the pipeline and to ensure that the same version of the pipeline is used consistently across different runs. The Nextflow info command can be used to view pipeline properties, such as the project name, repository, local path, main script, and revisions. The * indicates which revision of the pipeline you have stickied and will be executed when using the run command. nextflow info <pipeline> It is recommended that you use the revision flag every time you execute a pipeline to ensure that the version is correct. To use a specific revision, you simply need to add it to the command line with the --revision or -r flag. For example, to run a pipeline with the v1.0 revision, you would use the following: nextflow run <pipeline> -r v1.0 Nextflow automatically provides built-in support for version control using Git. With this, users can easily manage and track changes made to a pipeline over time. A revision can be a git branch , tag or commit SHA number, and can be used interchangeably. Exercise Execute the hello pipeline directly from the nextflow-io GitHub using the v1.1 revision tag. Solution Use the nextflow run command to execute the nextflow-io/hello pipeline with the v1.1 revision tag: nextflow run nextflow-io/hello -r v1.1 Output N E X T F L O W ~ version 23.04.0 NOTE: Your local project version looks outdated - a different revision is available in the remote repository [3b355db864] Nextflow DSL1 is no longer supported \u2014 Update your script to DSL2, or use Nextflow 22.10.x or earlier Warning The warning shown above is expected as the v1.1 pipeline revision was written using an older version of Nextflow that uses the depreciated echo method. As both Nextflow and pipelines are updated independently over time, pipelines and Nextflow functions can get out of sync. While most nf-core pipelines are now dsl2 (the current way of writing pipelines), some are still written in dsl1 and may require older version of Nextflow. You can use an older version of nextflow on the fly by adding adding the environmental variable to the start of the run command NXF_VER = 22 .10.0 nextflow run nextflow-io/hello -r v1.1 Output N E X T F L O W ~ version 22.10.0 NOTE: Your local project version looks outdated - a different revision is available in the remote repository [3b355db864] Launching `https://github.com/nextflow-io/hello` [amazing_lovelace] DSL1 - revision: baba3959d7 [v1.1] WARN: The use of `echo` method has been deprecated executor > local (4) [e6/cfda06] process > sayHello (4) [100%] 4 of 4 \u2714 Bojour world! (version 1.1) Hello world! (version 1.1) Ciao world! (version 1.1) Hola world! (version 1.1)","title":"Executing a revision"},{"location":"session_1/1_introToNextflow/#nextflow-log","text":"It is important to keep a record of the commands you have run to generate your results. Nextflow helps with this by creating and storing metadata and logs about the run in hidden files and folders in your current directory (unless otherwise specified). This data can be used by Nextflow to generate reports. It can also be queried using the Nextflow log command: nextflow log The log command has multiple options to facilitate the queries and is especially useful while debugging a pipeline and inspecting execution metadata. You can view all of the possible log options with -h flag: nextflow log -h To query a specific execution you can use the RUN NAME or a SESSION ID : nextflow log <run name> To get more information, you can use the -f option with named fields. For example: nextflow log <run name> -f process,hash,duration There are many other fields you can query. You can view a full list of fields with the -l option: nextflow log -l Exercise Use the log command to view with process , hash , and script fields for your tasks from your most recent Nextflow execution. Solution Use the log command to get a list of you recent executions: nextflow log Output TIMESTAMP DURATION RUN NAME STATUS REVISION ID SESSION ID COMMAND 2023-08-29 07:33:48 3.6s stupefied_bernard OK 1d71f857bb f9e18b71-d689-4589-be34-8cd98c1aab2e nextflow run nextflow-io/hello Query the process, hash, and script using the -f option for the most recent run: nextflow log stupefied_bernard -f process,hash,script Output sayHello f3/8f827f echo 'Hola world!' sayHello b8/b66545 echo 'Ciao world!' sayHello 3c/498a68 echo 'Bonjour world!' sayHello 6e/8d5b1a echo 'Hello world!'","title":"Nextflow log"},{"location":"session_1/1_introToNextflow/#execution-cache-and-resume","text":"Task execution caching is an essential feature of modern pipeline managers. Accordingly, Nextflow provides an automated caching mechanism for every execution. When using the Nextflow -resume option, successfully completed tasks from previous executions are skipped and the previously cached results are used in downstream tasks. Nextflow caching mechanism works by assigning a unique ID to each task. The task unique ID is generated as a 128-bit hash value composing the the complete file path, file size, and last modified timestamp. These ID's are used to create a separate execution directory where the tasks are executed and the outputs are stored. Nextflow will take care of the inputs and outputs in these folders for you. A multi-step pipeline is required to demonstrate cache and resume. The christopher-hakkaart/nf-core-demo pipeline was created with the nf-core create command and has the same structure as nf-core pipelines. It is a toy example with 3 processes: SAMPLESHEET_CHECK Executes a custom python script to check the input sample sheet is valid. FASTQC Executes FastQC using the .fastq.gz files from the sample sheet as inputs. MULTIQC Executes MultiQC using the FastQC reports generated by the FASTQC process. The christopher-hakkaartnf-core-demo is a very small nf-core pipeline. It uses real data and bioinformatics software and requires additional configuration to run successfully. To run this example you will need to include two profiles in your execution command. Profiles are sets of configuration options that can be accessed by Nextflow. Profiles will be explained in greater detail during the configuring nf-core pipelines section of the workshop. To run this pipeline, both the test profile and a software management profile (such as singularity ) are required: nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main This run requires 6GB of memory If your session was spawned with less than 8GB of memory, above run will fail with the following error Caused by: Process requirement exceeds available memory -- req: 6 GB; avail: 4 GB The command line output will print something like this: Output N E X T F L O W ~ version 23.04.0 Launching `https://github.com/christopher-hakkaart/nf-core-demo` [voluminous_kay] DSL2 - revision: 17521af3a8 [main] ------------------------------------------------------ ,--./,-. ___ __ __ __ ___ /,-._.--~' |\\ | |__ __ / ` / \\ |__) |__ } { | \\| | \\__, \\__/ | \\ |___ \\`-._,-`-, `._,._,' nf-core/demo v1.0dev-g17521af ------------------------------------------------------ Core Nextflow options revision : main runName : voluminous_kay containerEngine : singularity launchDir : /scale_wlg_persistent/filesets/home/chrishakk/session1 workDir : /scale_wlg_persistent/filesets/home/chrishakk/session1/work projectDir : /home/chrishakk/.nextflow/assets/christopher-hakkaart/nf-core-demo userName : chrishakk profile : test,singularity configFiles : /home/chrishakk/.nextflow/assets/christopher-hakkaart/nf-core-demo/nextflow.config Input/output options input : https://raw.githubusercontent.com/nf-core/test-datasets/viralrecon/samplesheet/samplesheet_test_illumina_amplicon.csv outdir : results Reference genome options genome : R64-1-1 fasta : s3://ngi-igenomes/igenomes/Saccharomyces_cerevisiae/Ensembl/R64-1-1/Sequence/WholeGenomeFasta/genome.fa Institutional config options config_profile_name : Test profile config_profile_description: Minimal test dataset to check pipeline function Max job request options max_cpus : 2 max_memory : 6.GB max_time : 6.h Generic options tracedir : null/pipeline_info !! Only displaying parameters that differ from the pipeline defaults !! ------------------------------------------------------ If you use nf-core/demo for your analysis please cite: * The nf-core framework https://doi.org/10.1038/s41587-020-0439-x * Software dependencies https://github.com/nf-core/demo/blob/master/CITATIONS.md ------------------------------------------------------ Downloading plugin nf-amazon@1.16.1 [f2/e5eb26] process > NFCORE_DEMO:DEMO:INPUT_CHECK:SAMPLESHEET_CHECK (samplesheet_test_illumina_amplicon.csv) [100%] 1 of 1 \u2714 [bb/f98425] process > NFCORE_DEMO:DEMO:FASTQC (SAMPLE1_PE_T1) [100%] 4 of 4 \u2714 [dd/728742] process > NFCORE_DEMO:DEMO:MULTIQC [100%] 1 of 1 \u2714 - Completed at: 29-Sep-2023 22:16:49 Duration : 2m 27s CPU hours : (a few seconds) Succeeded : 6 Executing this pipeline will create a work directory and a results directory with selected results files. In the output above, the hexadecimal numbers, such as bb/f98425 , identify the unique task execution. These numbers are also the prefix of the work directories where each task is executed. You can inspect the files produced by a task by looking inside the work directory and using these numbers to find the task-specific execution path: The files that have been selected for publication in the results folder can also be explored: ls results If you look inside the work directory of a FASTQC task, you will find the files that were staged and created when this task was executed: The FASTQC process runs four times, executing in a different work directories for each set of inputs. Therefore, in the previous example, the work directory [bb/f98425] represents just one of the four sets of input data that was processed. To print all the relevant paths to the screen, use the -ansi-log option can be used when executing your pipeline: nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main -ansi-log false It's very likely you will execute a pipeline multiple times as you find the parameters that best suit your data. You can save a lot of spaces (and time) if you resume a pipeline from the last step that was completed successfully or unmodified. By adding the -resume option to your run command you can use the cache rather than re-running successful tasks: nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main -resume If you run the christopher-hakkaart/nf-core-deme pipeline again without making any changes you will see that the cache is used: Output [truncated] [5f/07e477] process > NFCORE_DEMO:DEMO:INPUT_CHECK:SAMPLESHEET_CHECK (samplesheet_test_illumina_amplicon.csv) [100%] 1 of 1, cached: 1 \u2714 [b2/873706] process > NFCORE_DEMO:DEMO:FASTQC (SAMPLE2_PE_T1) [100%] 4 of 4, cached: 4 \u2714 [ca/e8e0a8] process > NFCORE_DEMO:DEMO:MULTIQC [100%] 1 of 1, cached: 1 \u2714 [truncated] In practical terms, the pipeline is executed from the beginning. However, before launching the execution of a process, Nextflow uses the task unique ID to check if the work directory already exists and that it contains a valid command exit state with the expected output files. If this condition is satisfied, the task execution is skipped and previously computed results are used as the process results. Notably, the -resume functionality is very sensitive. Even touching a file in the work directory can invalidate the cache. Exercise Invalidate the cache by touching a .fastq.gz file in a FASTQC task work directory (you can use the touch command). Execute the pipeline again with the -resume option to show that the cache has been invalidated. Solution Execute the pipeline for the first time (if you have not already). nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main Use the task ID shown for the FASTQC process and use it to find and touch the sample1_R1.fastq.gz file: touch work/b2/87370687cc7cdec037ce4f36807d32/sample1_R1.fastq.gz Execute the pipeline again with the -resume command option: nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main -resume You should that 2 of 4 tasks for FASTQC and the MULTIQC task were invalid and were executed again. Why did this happen? In this example, the cache of two FASTQC tasks were invalid. The sample1_R1.fastq.gz file is used by in the samplesheet twice. Thus, touching the symlink for this file and changing the date of last modification disrupted two task executions. Your work directory can get very big very quickly (especially if you are using full sized datasets). It is good practise to clean your work directory regularly. Rather than removing the work folder with all of it's contents, the Nextflow clean function allows you to selectively remove data associated with specific runs. nextflow clean -help The -after , -before , and -but options are all very useful to select specific runs to clean . The -dry-run option is also very useful to see which files will be removed if you were to -force the clean command. Exercise You Nextflow to clean your work work directory of staged files but keep your execution logs. Solution Use the Nextflow clean command with the -k and -f options: nextflow clean -k -f","title":"Execution cache and resume"},{"location":"session_1/1_introToNextflow/#listing-and-dropping-cached-pipelines","text":"Over time, you might want to remove a stored pipelines. Nextflow also has functionality to help you to view and remove pipelines that have been pulled locally. The Nextflow list command prints the projects stored in your global cache folder ( $HOME/.nextflow/assets ). These are the pipelines that were pulled when you executed either of the Nextflow pull or run commands: nextflow list If you want to remove a pipeline from your cache you can remove it using the Nextflow drop command: nextflow drop < pipeline> Exercise View your cached pipelines with the Nextflow list command and remove the nextflow-io/hello pipeline with the drop command. Solution List your pipeline assets: nextflow list Drop the nextflow-io/hello pipeline: nextflow drop nextflow-io/hello Check it has been removed: nextflow list Key points Nextflow is a pipeline orchestration engine and domain-specific language (DSL) that makes it easy to write data-intensive computational pipelines Environment variables can be used to control your Nextflow runtime and the underlying Java virtual machine Nextflow supports version control and has automatic integrations with online code repositories. Nextflow will cache your runs and they can be resumed with the -resume option You can manage pipelines with Nextflow commands (e.g., pull , clone , list , and drop )","title":"Listing and dropping cached pipelines"},{"location":"session_2/0_sessionOverview/","text":"Session two: nf-core and pipelines \u00b6 This session introduces the nf-core pipeline repository and provides an overview of the general structure of a pipeline. You will then access an existing pipeline, sarek, and execute this pipeline through Nextflow to analyse a small test dataset.","title":"Session two: nf-core and pipelines"},{"location":"session_2/0_sessionOverview/#session-two-nf-core-and-pipelines","text":"This session introduces the nf-core pipeline repository and provides an overview of the general structure of a pipeline. You will then access an existing pipeline, sarek, and execute this pipeline through Nextflow to analyse a small test dataset.","title":"Session two: nf-core and pipelines"},{"location":"session_2/1_introTo_nfCore/","text":"Introduction to nf-core \u00b6 Objectives Learn about the core features of nf-core. Learn how to use nf-core tooling. Use Nextflow to pull the nf-core/sarek workflow What is nf-core? \u00b6 nf-core is a community effort to collect a curated set of analysis workflows built using Nextflow. nf-core provides a standardised set of best practices , guidelines , and templates for building and sharing bioinformatics workflows. These workflows are designed to be modular , scalable , and portable , allowing researchers to easily adapt and execute them using their own data and compute resources. The community is a diverse group of bioinformaticians, developers, and researchers from around the world who collaborate on developing and maintaining a growing collection of high-quality workflows. These workflows cover a range of applications, including transcriptomics, proteomics, and metagenomics. One of the key benefits of nf-core is that it promotes open development , testing , and peer review , ensuring that the workflows are robust, well-documented, and validated against real-world datasets. This helps to increase the reliability and reproducibility of bioinformatics analyses and ultimately enables researchers to accelerate their scientific discoveries. nf-core is published in Nature Biotechnology: Nat Biotechnol 38, 276\u2013278 (2020). Nature Biotechnology Key Features of nf-core workflows Documentation nf-core workflows have extensive documentation covering installation, usage, and description of output files to ensure that you won't be left in the dark. CI Testing Every time a change is made to the workflow code, nf-core workflows use continuous-integration testing to ensure that nothing has broken. Stable Releases nf-core workflows use GitHub releases to tag stable versions of the code and software, making workflow runs totally reproducible. Packaged software Pipeline dependencies are automatically downloaded and handled using Docker, Singularity, Conda, or other software management tools. There is no need for any software installations. Portable and reproducible nf-core workflows follow best practices to ensure maximum portability and reproducibility. The large community makes the workflows exceptionally well-tested and easy to execute. Cloud-ready nf-core workflows are tested on AWS after every major release. You can even browse results live on the website and use outputs for your own benchmarking. It is important to remember all nf-core workflows are open-source and community driven . Pipelines are under active community development and are regularly updated with fixes and other improvements. Even though the pipelines and tools undergo repeated community review and testing - it is important to check your results. Events \u00b6 nf-core events are community-driven gatherings that provide a platform to discuss the latest developments in Nextflow and nf-core workflows. These events include community seminars , trainings , and hackathons , and are open to anyone who is interested in using and developing nf-core and its applications. Most events are held virtually, making them accessible to a global audience. Upcoming events are listed on the nf-core event page and announced on Slack and Twitter . Join the community! \u00b6 There are several ways you can join the nf-core community. You are welcome to join any or all of these at any time! The nf-core Slack is one of the primary resources for nf-core users. There are dedicated channels for all workflows as well as channels for common topics. If you are unsure of where to ask you questions - the #help and #nostupidquestions channels are a great place to start. Questions about Nextflow If you have questions about Nextflow and deployments that are not related to nf-core you can ask them on the Nextflow Slack . It's worthwhile joining both Slack groups and browsing the channels to get an idea of what types of questions are being asked on each channel. Searching channels can also be a great source of information as your question may have been asked before. Joining multiple nf-core and Nextflow channels is important to keep up to date with the latest community developments and updates. In particular, following the nf-core and Nextflow Twitter accounts will keep you up-to-date with community announcements. If you are looking for more information about a workflow, the nf-core YouTube channel regularly shares ByteSize seminars about best practises, workflows, and community developments. Exercise Join the nf-core Slack and fill in your profile information. If you're joining the nf-core Slack for the first time make sure you drop a message in #say-hello to introduce yourself! \ud83d\udc4b Solution Follow this link to join the nf-core Slack. Follow the instructions to enter your credentials and update your profile. Even if you are already a member of the nf-core Slack, it's a great time to check your profile is up-to-date. nf-core tools \u00b6 This workshop will make use of nf-core tools, a set of helper tools for use with Nextflow workflows. These tools have been developed to provide a range of additional functionality for using , developing , and testing workflows. How to download nf-core tools - Don't have to do this install today as it is already installed to your workshop environment. nf-core tools is written in Python and is available from the Python Package Index (PyPI) : pip install nf-core Alternatively, nf-core tools can be installed from Bioconda : conda install -c bioconda nf-core The nf-core --version option can be used to print your version of nf-core tools: nf-core --version Exercise Find out what version of nf-core tools you have available using the nf-core --version option. If nf-core tools is not installed then install it using the commands above: Solution Use the nf-core --version option to print your nf-core tools version: nf-core --version If you get the message \" nf-core: command not found \" - install nf-core using the commands above: Download nf-core from the Python Package Index (PyPI): pip install nf-core Use the nf-core --version option to print your nf-core tools version: nf-core --version Warning Some of these commands may change depending on the operating system you are using. nf-core tools are for everyone and has commands to help both users and developers . For users, the tools make it easier to execute workflows. For developers, the tools make it easier to develop and test your workflows using best practices. You can read about the nf-core commands on the tools page of the nf-core website or using the command line. Exercise Find out what nf-core tools commands and options are available using the --help option: Solution Execute the --help option to list the options, commands for users, and commands for developers: nf-core --help nf-core tools is updated with new features and fixes regularly so it's best to keep your version of nf-core tools up-to-date. Executing an nf-core workflow \u00b6 There are currently 88 workflows (September 2023) available as part of nf-core. These workflows are at various stages of development with 53 released, 23 under development, and 12 archived. The nf-core website has a full list of workflows, as well as their documentation, which can be explored. Each workflow has a dedicated page that includes expansive documentation that is split into 6 sections: Introduction: An introduction and overview of the workflow Usage: Descriptions of how to execute the workflow Parameters: Grouped workflow parameters with descriptions Output: Descriptions and examples of the expected output files Results: Example output files generated from the full test dataset Releases & Statistics: Workflow version history and statistics Unless you are actively developing workflow code, you don't need to clone the workflow code from GitHub and can use Nextflow\u2019s built-in functionality to pull and a workflow. As shown in the previous lesson, the Nextflow pull command can download and cache workflows from GitHub repositories: nextflow pull nf-core/<pipeline> Nextflow run will also automatically pull the workflow if it was not already available locally: nextflow run nf-core/<pipeline> Nextflow will pull the default git branch if a workflow version is not specified. This will be the master branch for nf-core workflows with a stable release. nf-core workflows use GitHub releases to tag stable versions of the code and software. You will always be able to execute a previous version of a workflow once it is released using the -revision or -r flag. Exercise Use Nextflow to pull the latest version of the nf-core/sarek workflow directly from GitHub: Solution Use Nextlfow to pull the sarek workflow from the nf-core GitHub repository: nextflow pull nf-core/sarek -r 3 .2.3 Check that it has been pulled by listing your cached pipelines: nextflow list Key points nf-core is a community effort to collect a curated set of analysis workflows built using Nextflow. You can join/follow nf-core on multiple different social channels (Slack, YouTube, Twitter...) nf-core has its own tooling that can be used by users and developers. Nextflow can be used to pull nf-core workflows.","title":"Introduction to nf-core"},{"location":"session_2/1_introTo_nfCore/#introduction-to-nf-core","text":"Objectives Learn about the core features of nf-core. Learn how to use nf-core tooling. Use Nextflow to pull the nf-core/sarek workflow","title":"Introduction to nf-core"},{"location":"session_2/1_introTo_nfCore/#what-is-nf-core","text":"nf-core is a community effort to collect a curated set of analysis workflows built using Nextflow. nf-core provides a standardised set of best practices , guidelines , and templates for building and sharing bioinformatics workflows. These workflows are designed to be modular , scalable , and portable , allowing researchers to easily adapt and execute them using their own data and compute resources. The community is a diverse group of bioinformaticians, developers, and researchers from around the world who collaborate on developing and maintaining a growing collection of high-quality workflows. These workflows cover a range of applications, including transcriptomics, proteomics, and metagenomics. One of the key benefits of nf-core is that it promotes open development , testing , and peer review , ensuring that the workflows are robust, well-documented, and validated against real-world datasets. This helps to increase the reliability and reproducibility of bioinformatics analyses and ultimately enables researchers to accelerate their scientific discoveries. nf-core is published in Nature Biotechnology: Nat Biotechnol 38, 276\u2013278 (2020). Nature Biotechnology Key Features of nf-core workflows Documentation nf-core workflows have extensive documentation covering installation, usage, and description of output files to ensure that you won't be left in the dark. CI Testing Every time a change is made to the workflow code, nf-core workflows use continuous-integration testing to ensure that nothing has broken. Stable Releases nf-core workflows use GitHub releases to tag stable versions of the code and software, making workflow runs totally reproducible. Packaged software Pipeline dependencies are automatically downloaded and handled using Docker, Singularity, Conda, or other software management tools. There is no need for any software installations. Portable and reproducible nf-core workflows follow best practices to ensure maximum portability and reproducibility. The large community makes the workflows exceptionally well-tested and easy to execute. Cloud-ready nf-core workflows are tested on AWS after every major release. You can even browse results live on the website and use outputs for your own benchmarking. It is important to remember all nf-core workflows are open-source and community driven . Pipelines are under active community development and are regularly updated with fixes and other improvements. Even though the pipelines and tools undergo repeated community review and testing - it is important to check your results.","title":"What is nf-core?"},{"location":"session_2/1_introTo_nfCore/#events","text":"nf-core events are community-driven gatherings that provide a platform to discuss the latest developments in Nextflow and nf-core workflows. These events include community seminars , trainings , and hackathons , and are open to anyone who is interested in using and developing nf-core and its applications. Most events are held virtually, making them accessible to a global audience. Upcoming events are listed on the nf-core event page and announced on Slack and Twitter .","title":"Events"},{"location":"session_2/1_introTo_nfCore/#join-the-community","text":"There are several ways you can join the nf-core community. You are welcome to join any or all of these at any time! The nf-core Slack is one of the primary resources for nf-core users. There are dedicated channels for all workflows as well as channels for common topics. If you are unsure of where to ask you questions - the #help and #nostupidquestions channels are a great place to start. Questions about Nextflow If you have questions about Nextflow and deployments that are not related to nf-core you can ask them on the Nextflow Slack . It's worthwhile joining both Slack groups and browsing the channels to get an idea of what types of questions are being asked on each channel. Searching channels can also be a great source of information as your question may have been asked before. Joining multiple nf-core and Nextflow channels is important to keep up to date with the latest community developments and updates. In particular, following the nf-core and Nextflow Twitter accounts will keep you up-to-date with community announcements. If you are looking for more information about a workflow, the nf-core YouTube channel regularly shares ByteSize seminars about best practises, workflows, and community developments. Exercise Join the nf-core Slack and fill in your profile information. If you're joining the nf-core Slack for the first time make sure you drop a message in #say-hello to introduce yourself! \ud83d\udc4b Solution Follow this link to join the nf-core Slack. Follow the instructions to enter your credentials and update your profile. Even if you are already a member of the nf-core Slack, it's a great time to check your profile is up-to-date.","title":"Join the community!"},{"location":"session_2/1_introTo_nfCore/#nf-core-tools","text":"This workshop will make use of nf-core tools, a set of helper tools for use with Nextflow workflows. These tools have been developed to provide a range of additional functionality for using , developing , and testing workflows. How to download nf-core tools - Don't have to do this install today as it is already installed to your workshop environment. nf-core tools is written in Python and is available from the Python Package Index (PyPI) : pip install nf-core Alternatively, nf-core tools can be installed from Bioconda : conda install -c bioconda nf-core The nf-core --version option can be used to print your version of nf-core tools: nf-core --version Exercise Find out what version of nf-core tools you have available using the nf-core --version option. If nf-core tools is not installed then install it using the commands above: Solution Use the nf-core --version option to print your nf-core tools version: nf-core --version If you get the message \" nf-core: command not found \" - install nf-core using the commands above: Download nf-core from the Python Package Index (PyPI): pip install nf-core Use the nf-core --version option to print your nf-core tools version: nf-core --version Warning Some of these commands may change depending on the operating system you are using. nf-core tools are for everyone and has commands to help both users and developers . For users, the tools make it easier to execute workflows. For developers, the tools make it easier to develop and test your workflows using best practices. You can read about the nf-core commands on the tools page of the nf-core website or using the command line. Exercise Find out what nf-core tools commands and options are available using the --help option: Solution Execute the --help option to list the options, commands for users, and commands for developers: nf-core --help nf-core tools is updated with new features and fixes regularly so it's best to keep your version of nf-core tools up-to-date.","title":"nf-core tools"},{"location":"session_2/1_introTo_nfCore/#executing-an-nf-core-workflow","text":"There are currently 88 workflows (September 2023) available as part of nf-core. These workflows are at various stages of development with 53 released, 23 under development, and 12 archived. The nf-core website has a full list of workflows, as well as their documentation, which can be explored. Each workflow has a dedicated page that includes expansive documentation that is split into 6 sections: Introduction: An introduction and overview of the workflow Usage: Descriptions of how to execute the workflow Parameters: Grouped workflow parameters with descriptions Output: Descriptions and examples of the expected output files Results: Example output files generated from the full test dataset Releases & Statistics: Workflow version history and statistics Unless you are actively developing workflow code, you don't need to clone the workflow code from GitHub and can use Nextflow\u2019s built-in functionality to pull and a workflow. As shown in the previous lesson, the Nextflow pull command can download and cache workflows from GitHub repositories: nextflow pull nf-core/<pipeline> Nextflow run will also automatically pull the workflow if it was not already available locally: nextflow run nf-core/<pipeline> Nextflow will pull the default git branch if a workflow version is not specified. This will be the master branch for nf-core workflows with a stable release. nf-core workflows use GitHub releases to tag stable versions of the code and software. You will always be able to execute a previous version of a workflow once it is released using the -revision or -r flag. Exercise Use Nextflow to pull the latest version of the nf-core/sarek workflow directly from GitHub: Solution Use Nextlfow to pull the sarek workflow from the nf-core GitHub repository: nextflow pull nf-core/sarek -r 3 .2.3 Check that it has been pulled by listing your cached pipelines: nextflow list Key points nf-core is a community effort to collect a curated set of analysis workflows built using Nextflow. You can join/follow nf-core on multiple different social channels (Slack, YouTube, Twitter...) nf-core has its own tooling that can be used by users and developers. Nextflow can be used to pull nf-core workflows.","title":"Executing an nf-core workflow"},{"location":"session_2/2_pipelineStructure/","text":"Pipeline structure \u00b6 Objectives Learn about the structure of an nf-core pipeline. Pipeline structure \u00b6 nf-core pipelines follow a set of best practices and standardised conventions. nf-core pipelines start from a common template and follow the same structure. Although you won\u2019t need to edit code in the pipeline project directory, having a basic understanding of the project structure and some core terminology will help you understand how to configure its execution. Nextflow DSL2 workflow are built up of subworkflows and modules that are stored as separate .nf files. Most nf-core pipelines consist of a single workflow file (there are a few exceptions). This is the main <workflow>.nf file that is used to bring everything else together. Instead of having one large monolithic script, it is broken up into a combination of subworkflows and modules . A subworkflows is a groups of modules that are used in combination with each other and have a common purpose. For example, the SAMTOOLS_STATS , SAMTOOLS_IDXSTATS , and SAMTOOLS_FLAGSTAT modules are all included in the BAM_STATS_SAMTOOLS subworkflow. Subworkflows improve pipeline readability and help with the reuse of modules within a pipeline. Within a nf-core pipeline, a subworkflow can be an nf-core subworkflow or as a local subworkflow. Like an nf-core pipeline, an nf-core subworkflow is developed by the community is shared in the nf-core subworkflows GitHub repository . Local subworkflows are pipeline specific that are not shared in the nf-core subworkflows repository. A modules is a wrapper for a process, the basic processing primitive to execute a user script. It can specify directives , inputs , outputs , when statements , and a script block. Most modules will execute a single tool in the script block and will make use of the directives, inputs, outputs, and when statements dynamically. Like subworkflows, modules can also be developed and shared in the nf-core modules GitHub repository or stored as a local module. All modules from the nf-core repository are version controlled and tested to ensure reproducibility. Local modules are pipeline specific that are not shared in the nf-core modules repository.","title":"Pipeline structure"},{"location":"session_2/2_pipelineStructure/#pipeline-structure","text":"Objectives Learn about the structure of an nf-core pipeline.","title":"Pipeline structure"},{"location":"session_2/2_pipelineStructure/#pipeline-structure_1","text":"nf-core pipelines follow a set of best practices and standardised conventions. nf-core pipelines start from a common template and follow the same structure. Although you won\u2019t need to edit code in the pipeline project directory, having a basic understanding of the project structure and some core terminology will help you understand how to configure its execution. Nextflow DSL2 workflow are built up of subworkflows and modules that are stored as separate .nf files. Most nf-core pipelines consist of a single workflow file (there are a few exceptions). This is the main <workflow>.nf file that is used to bring everything else together. Instead of having one large monolithic script, it is broken up into a combination of subworkflows and modules . A subworkflows is a groups of modules that are used in combination with each other and have a common purpose. For example, the SAMTOOLS_STATS , SAMTOOLS_IDXSTATS , and SAMTOOLS_FLAGSTAT modules are all included in the BAM_STATS_SAMTOOLS subworkflow. Subworkflows improve pipeline readability and help with the reuse of modules within a pipeline. Within a nf-core pipeline, a subworkflow can be an nf-core subworkflow or as a local subworkflow. Like an nf-core pipeline, an nf-core subworkflow is developed by the community is shared in the nf-core subworkflows GitHub repository . Local subworkflows are pipeline specific that are not shared in the nf-core subworkflows repository. A modules is a wrapper for a process, the basic processing primitive to execute a user script. It can specify directives , inputs , outputs , when statements , and a script block. Most modules will execute a single tool in the script block and will make use of the directives, inputs, outputs, and when statements dynamically. Like subworkflows, modules can also be developed and shared in the nf-core modules GitHub repository or stored as a local module. All modules from the nf-core repository are version controlled and tested to ensure reproducibility. Local modules are pipeline specific that are not shared in the nf-core modules repository.","title":"Pipeline structure"},{"location":"session_2/3_sarekPipeline/","text":"The Sarek pipeline \u00b6 Objectives Understand the Sarek pipeline structure and default usage Understand the levels of customisation available for nf-core pipelines Use the nf-core documentation to select appropriate parameters for a run command Write and run a nf-core sarek command on the command line The Sarek pipeline \u00b6 nf-core/sarek is a pipeline designed to detect variants on whole genome or targeted sequencing data. Initially designed for Human, and Mouse, it can work on any species with a reference genome. Sarek can also handle tumour/normal pairs and could include additional relapses. The pipeline makes use of Docker/Singularity containers, making installation trivial and results highly reproducible. The Nextflow DSL2 implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Depending on the options and samples provided, the pipeline can currently perform the following: Form consensus reads from UMI sequences ( fgbio ) Sequencing quality control and trimming ( FastQC , fastp ) Map Reads to Reference ( BWA-mem or BWA-mem2 or dragmap ) Process BAM file ( GATK MarkDuplicates , GATK BaseRecalibrator , GATK ApplyBQSR ) Summarise alignment statistics ( samtools stats , mosdepth ) Variant calling (enabled by --tools , see compatibility ): HaplotypeCaller , freebayes , mpileup , Strelka2 , DeepVariant , Mutect2 , Manta , TIDDIT , ASCAT , Control-FREEC , CNVkit , and / or MSIsensor-pro Variant filtering and annotation ( SnpEff , Ensembl VEP ) Summarise and represent QC ( MultiQC ) nf-core pipelines are frequently represented as subway maps. The nf-core/sarek subway map is shown below and is a good place to start when first understanding how the pipeline works. Download the Sarek pipeline \u00b6 There are multiple ways you can download and store a copy of a nf-core pipeline. Firstly, you could use the nextflow pull command. By default, if you you the nextflow run command to execute a pipeline from github it will also pull the pipeline. In both of these cases the pipeline will be stored in a hidden directory in your home directory. Secondly, you could clone a copy of the pipeline using the standard git clone command, e.g., git clone https://github.com/nf-core/sarek.git . This will download the pipeline to your current working directory. Finally, you could use the nf-core download utility to download a copy of the pipeline. This will give the the option to download the pipeline code, the required singularity images, and the institutional configs from the nf-core github repository. This method can be especially helpful if you are working offline and want to move all of the pipeline code and tooling to a different machine. Getting started \u00b6 All nf-core pipelines are provided with comprehensive documentation that explain what the default pipeline structure entails and options for customising this based on your needs. It is important to remember that nf-core pipelines typically do not include all possible tool parameters. Instead, they provide a sensible set of parameters that are suitable for most use cases. The number and type of parameters an nf-core pipeline accepts differ between pipelines. The recommended (typical) run command and all the parameters available for the nf-core/sarek pipeline can be viewed using the --help flag: nextflow run nf-core/sarek -r 3 .2.3 --help Revision 3.2.3 The Sarek pipeline is always improving but we want to ensure that the results of this workshop are reproducible. To ensure this, we will use a specific version (3.2.3) of the pipeline and the revision flag ( -r ). At the top of the help output, you will see the recommended run command: nextflow run nf-core/sarek --input samplesheet.csv --genome GATK.GRCh38 -profile docker It outlines the requirement for three things: An input samplesheet ( --input ) A reference genome ( --genome ) A software management profile ( --profile ) Hyphens matter Nextflow-specific parameters use one ( - ) hyphen, whereas pipeline-specific parameters use two ( -- ). More information about Sarek \u00b6 There is extensive information about nf-core pipelines on the nf-core website . The dedicated Sarek pipeline page is the best resource for information about the pipeline and how to execute it. If you have specific questions that are not included in the documentation you can join the nf-core Slack workspace and ask in the #sarek channel. Testing a pipeline \u00b6 Before running a pipeline on your own data, it is a good idea to test the pipeline on a small dataset. This allows you to check that the pipeline is working as expected without having to wait for a long time for the pipeline to complete. The test profile will run the pipeline on a small test dataset that is included with the pipeline code. nextflow run nf-core/sarek -profile test,singularity --outdir test_sarek -r 3 .2.3 Exercise Check that Sarek is working by running the pipeline with the test profile. Solution Run the test profile: nextflow run nf-core/sarek -profile test,singularity --outdir test_sarek -r 3 .2.3 Test data The --input and --genome parameters are not required when using the test profile . The test data and small reference files are included with the pipeline code and are automatically used when the test profile is specified. Key points nf-core pipelines are provided with sensible default settings and required inputs. The --help flag can be used to view the recommended run command and all available parameters. The test profile can be used to show that a pipeline is working as expected.","title":"The Sarek pipeline"},{"location":"session_2/3_sarekPipeline/#the-sarek-pipeline","text":"Objectives Understand the Sarek pipeline structure and default usage Understand the levels of customisation available for nf-core pipelines Use the nf-core documentation to select appropriate parameters for a run command Write and run a nf-core sarek command on the command line","title":"The Sarek pipeline"},{"location":"session_2/3_sarekPipeline/#the-sarek-pipeline_1","text":"nf-core/sarek is a pipeline designed to detect variants on whole genome or targeted sequencing data. Initially designed for Human, and Mouse, it can work on any species with a reference genome. Sarek can also handle tumour/normal pairs and could include additional relapses. The pipeline makes use of Docker/Singularity containers, making installation trivial and results highly reproducible. The Nextflow DSL2 implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Depending on the options and samples provided, the pipeline can currently perform the following: Form consensus reads from UMI sequences ( fgbio ) Sequencing quality control and trimming ( FastQC , fastp ) Map Reads to Reference ( BWA-mem or BWA-mem2 or dragmap ) Process BAM file ( GATK MarkDuplicates , GATK BaseRecalibrator , GATK ApplyBQSR ) Summarise alignment statistics ( samtools stats , mosdepth ) Variant calling (enabled by --tools , see compatibility ): HaplotypeCaller , freebayes , mpileup , Strelka2 , DeepVariant , Mutect2 , Manta , TIDDIT , ASCAT , Control-FREEC , CNVkit , and / or MSIsensor-pro Variant filtering and annotation ( SnpEff , Ensembl VEP ) Summarise and represent QC ( MultiQC ) nf-core pipelines are frequently represented as subway maps. The nf-core/sarek subway map is shown below and is a good place to start when first understanding how the pipeline works.","title":"The Sarek pipeline"},{"location":"session_2/3_sarekPipeline/#download-the-sarek-pipeline","text":"There are multiple ways you can download and store a copy of a nf-core pipeline. Firstly, you could use the nextflow pull command. By default, if you you the nextflow run command to execute a pipeline from github it will also pull the pipeline. In both of these cases the pipeline will be stored in a hidden directory in your home directory. Secondly, you could clone a copy of the pipeline using the standard git clone command, e.g., git clone https://github.com/nf-core/sarek.git . This will download the pipeline to your current working directory. Finally, you could use the nf-core download utility to download a copy of the pipeline. This will give the the option to download the pipeline code, the required singularity images, and the institutional configs from the nf-core github repository. This method can be especially helpful if you are working offline and want to move all of the pipeline code and tooling to a different machine.","title":"Download the Sarek pipeline"},{"location":"session_2/3_sarekPipeline/#getting-started","text":"All nf-core pipelines are provided with comprehensive documentation that explain what the default pipeline structure entails and options for customising this based on your needs. It is important to remember that nf-core pipelines typically do not include all possible tool parameters. Instead, they provide a sensible set of parameters that are suitable for most use cases. The number and type of parameters an nf-core pipeline accepts differ between pipelines. The recommended (typical) run command and all the parameters available for the nf-core/sarek pipeline can be viewed using the --help flag: nextflow run nf-core/sarek -r 3 .2.3 --help Revision 3.2.3 The Sarek pipeline is always improving but we want to ensure that the results of this workshop are reproducible. To ensure this, we will use a specific version (3.2.3) of the pipeline and the revision flag ( -r ). At the top of the help output, you will see the recommended run command: nextflow run nf-core/sarek --input samplesheet.csv --genome GATK.GRCh38 -profile docker It outlines the requirement for three things: An input samplesheet ( --input ) A reference genome ( --genome ) A software management profile ( --profile ) Hyphens matter Nextflow-specific parameters use one ( - ) hyphen, whereas pipeline-specific parameters use two ( -- ).","title":"Getting started"},{"location":"session_2/3_sarekPipeline/#more-information-about-sarek","text":"There is extensive information about nf-core pipelines on the nf-core website . The dedicated Sarek pipeline page is the best resource for information about the pipeline and how to execute it. If you have specific questions that are not included in the documentation you can join the nf-core Slack workspace and ask in the #sarek channel.","title":"More information about Sarek"},{"location":"session_2/3_sarekPipeline/#testing-a-pipeline","text":"Before running a pipeline on your own data, it is a good idea to test the pipeline on a small dataset. This allows you to check that the pipeline is working as expected without having to wait for a long time for the pipeline to complete. The test profile will run the pipeline on a small test dataset that is included with the pipeline code. nextflow run nf-core/sarek -profile test,singularity --outdir test_sarek -r 3 .2.3 Exercise Check that Sarek is working by running the pipeline with the test profile. Solution Run the test profile: nextflow run nf-core/sarek -profile test,singularity --outdir test_sarek -r 3 .2.3 Test data The --input and --genome parameters are not required when using the test profile . The test data and small reference files are included with the pipeline code and are automatically used when the test profile is specified. Key points nf-core pipelines are provided with sensible default settings and required inputs. The --help flag can be used to view the recommended run command and all available parameters. The test profile can be used to show that a pipeline is working as expected.","title":"Testing a pipeline"},{"location":"session_2/4_runConfiguration/","text":"Configuring your run \u00b6 Objectives Understand the different parts of a run command Learn how to customise a run command Learn how to use a parameter file and configuration files to customise a run command Where to start \u00b6 A recommended run command can be found for each pipeline on the nf-core website and is a useful starting point for customising a run command: nextflow run nf-core/sarek --input samplesheet.csv --genome GATK.GRCh38 -profile docker From here, the command can be customised to suit your needs. The following sections will describe the different components of the command and how they can be customised. In this example, the same small test data files that are used in the test profile will be used to demonstrate how to create you own sample sheet. However, you will be writing the files rather than relying on the test profile. The same concepts will apply to data on your local storage. Input ( --input ) \u00b6 The Sarek pipeline requires a samplesheet as an input parameter. This is a csv file that contains information about the samples that will be processed. The samplesheet is used to specify the location of the input data, the sample name, and additional metadata. A samplesheet is created manually and can be stored anywhere on your computer. The samplesheet can be named anything you like, but it must be specified using the --input flag. More information about how to structure a samplesheet for Sarek can be found in the usage documentation . Note how Sarek can accept different data types as inputs and how the samplesheet is different for each. Exercise Use the usage documentation to create samplesheet.csv in your working directory. It must include the following data in the required format: patient: test sex: XX status: 0 sample: test lane: test_L1 fastq_1: https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_1.fastq.gz fastq_2: https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_2.fastq.gz Solution Your csv file should look like the following: samplesheet.csv patient,sex,status,sample,lane,fastq_1,fastq_2 test,XX,0,test,test_L1,https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_1.fastq.gz,https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_2.fastq.gz Reference data ( --genome ) \u00b6 Many nf-core pipelines need a reference genome for alignment, annotation, or similar. To make the use of reference genomes easier, Illumina developed a centralised resource called iGenomes where the most commonly used reference genome files are organised in a consistent structure. nf-core have uploaded a copy of iGenomes onto AWS S3 and nf-core pipelines are configured to use this by default. All AWS iGenomes paths are specified in pipelines that support them in conf/igenomes.config . By default, the pipeline will automatically download the required reference files when you it is executed with an appropriate genome key (e.g., --genome GRCh37 ). The pipeline will only download what it requires. Downloading reference genome files takes time and bandwidth so, if possible, it is recommend that you download a local copy of your relevant iGenomes references and configure your execution to use the local version. When executing Sarek with common genomes, such as GRCh38 and GRCh37, iGenomes is shipped with the necessary reference files. However, depending on your deployment, it is sometimes necessary to use custom references for some or all files. Specific details for different deployment situations are described in the usage documentation . The small test fastq.gz files in the samplesheet created above would throw errors if it was run with a full size reference genome from iGenomes. Instead, smaller files from the nf-core test datasets repository need to be used. As these files are not included in iGenomes they must be specified manually using parameters. The following parameters can be used to specify the required reference/annotation files for the small test data files: --igenomes_ignore \\ --dbsnp \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz\" \\ --fasta \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.fasta\" \\ --germline_resource \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/gnomAD.r2.1.1.vcf.gz\" \\ --intervals \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.interval_list\" \\ --known_indels \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz\" \\ --snpeff_db 105 \\ --snpeff_genome \"WBcel235\" \\ --snpeff_version \"5.1\" \\ --vep_cache_version \"106\" \\ --vep_genome \"WBcel235\" \\ --vep_species \"caenorhabditis_elegans\" \\ --vep_version \"106.1\" \\ --max_cpus 2 \\ --max_memory 6.5GB \\ --tools \"freebayes\" \\ --outdir \"my_results\" Tools The --tools parameter is included above to trigger the execution of the freebayes variant caller. Multiple variant callers are available as a part of Sarek, however, in this example, only one is included. Warning The --igenomes_ignore parameter must be included when using custom reference/annotation files. Without it, by default, the reference/annotation files that are typically required by Sarek are downloaded for GATK.GRCh38 . Some pipelines have this feature by default while others require the genome flag (or alternate) for every execution. Profiles ( --profile ) \u00b6 Software profiles are used to specify the software environment in which the pipeline will be executed. By simply including a profile (e.g., singularity ), Nextflow will download, store, and manage the software used in the Sarek pipeline. To ensure reproducibility, it is recommended that you use container technology, e.g., docker or singularity . Putting it all together \u00b6 The previous sections have shown different parts of a recommended run command. These can be combined to create a custom run command that will execute Sarek on the small test data files. The completed run command will execute a small test set of files using the freebayes variant caller. The command will use the small test data files from the nf-core test datasets repository and custom reference/annotation files that are hosted on github. Exercise Use all of the information above to build a custom run command that will execute version 3.2.3 of Sarek on the samplesheet you created. Hint Include the sample sheet you made earlier with --input samplesheet.csv . Remember is must be in your working directory or you must specify the full or relative path to the file. If you named your samplesheet differently it must be reflected in your command. Include the parameters shown above to specify the reference/annotation files. You can copy these directly into your run command. Use Singularity to manage your software with -profile singularity . Solution Your run command should look like the following: nextflow run nf-core/sarek \\ --input samplesheet.csv \\ --igenomes_ignore \\ --dbsnp \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz\" \\ --fasta \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.fasta\" \\ --germline_resource \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/gnomAD.r2.1.1.vcf.gz\" \\ --intervals \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.interval_list\" \\ --known_indels \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz\" \\ --snpeff_db 105 \\ --snpeff_genome \"WBcel235\" \\ --snpeff_version \"5.1\" \\ --vep_cache_version \"106\" \\ --vep_genome \"WBcel235\" \\ --vep_species \"caenorhabditis_elegans\" \\ --vep_version \"106.1\" \\ --max_cpus 2 \\ --max_memory 6 .5GB \\ --tools \"freebayes\" \\ --outdir \"my_results\" \\ -profile singularity \\ -r 3 .2.3 If everything has worked - you will see the pipeline launching in your terminal \ud83d\ude80 Key points Sarek comes with a test profiles that can be used to test the pipeline on your infrastructure Sample sheets are csv files that contain important meta data and the paths to your files Reference files are available from iGenomes","title":"Configuring your run"},{"location":"session_2/4_runConfiguration/#configuring-your-run","text":"Objectives Understand the different parts of a run command Learn how to customise a run command Learn how to use a parameter file and configuration files to customise a run command","title":"Configuring your run"},{"location":"session_2/4_runConfiguration/#where-to-start","text":"A recommended run command can be found for each pipeline on the nf-core website and is a useful starting point for customising a run command: nextflow run nf-core/sarek --input samplesheet.csv --genome GATK.GRCh38 -profile docker From here, the command can be customised to suit your needs. The following sections will describe the different components of the command and how they can be customised. In this example, the same small test data files that are used in the test profile will be used to demonstrate how to create you own sample sheet. However, you will be writing the files rather than relying on the test profile. The same concepts will apply to data on your local storage.","title":"Where to start"},{"location":"session_2/4_runConfiguration/#input-input","text":"The Sarek pipeline requires a samplesheet as an input parameter. This is a csv file that contains information about the samples that will be processed. The samplesheet is used to specify the location of the input data, the sample name, and additional metadata. A samplesheet is created manually and can be stored anywhere on your computer. The samplesheet can be named anything you like, but it must be specified using the --input flag. More information about how to structure a samplesheet for Sarek can be found in the usage documentation . Note how Sarek can accept different data types as inputs and how the samplesheet is different for each. Exercise Use the usage documentation to create samplesheet.csv in your working directory. It must include the following data in the required format: patient: test sex: XX status: 0 sample: test lane: test_L1 fastq_1: https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_1.fastq.gz fastq_2: https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_2.fastq.gz Solution Your csv file should look like the following: samplesheet.csv patient,sex,status,sample,lane,fastq_1,fastq_2 test,XX,0,test,test_L1,https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_1.fastq.gz,https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/illumina/fastq/test_2.fastq.gz","title":"Input (--input)"},{"location":"session_2/4_runConfiguration/#reference-data-genome","text":"Many nf-core pipelines need a reference genome for alignment, annotation, or similar. To make the use of reference genomes easier, Illumina developed a centralised resource called iGenomes where the most commonly used reference genome files are organised in a consistent structure. nf-core have uploaded a copy of iGenomes onto AWS S3 and nf-core pipelines are configured to use this by default. All AWS iGenomes paths are specified in pipelines that support them in conf/igenomes.config . By default, the pipeline will automatically download the required reference files when you it is executed with an appropriate genome key (e.g., --genome GRCh37 ). The pipeline will only download what it requires. Downloading reference genome files takes time and bandwidth so, if possible, it is recommend that you download a local copy of your relevant iGenomes references and configure your execution to use the local version. When executing Sarek with common genomes, such as GRCh38 and GRCh37, iGenomes is shipped with the necessary reference files. However, depending on your deployment, it is sometimes necessary to use custom references for some or all files. Specific details for different deployment situations are described in the usage documentation . The small test fastq.gz files in the samplesheet created above would throw errors if it was run with a full size reference genome from iGenomes. Instead, smaller files from the nf-core test datasets repository need to be used. As these files are not included in iGenomes they must be specified manually using parameters. The following parameters can be used to specify the required reference/annotation files for the small test data files: --igenomes_ignore \\ --dbsnp \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz\" \\ --fasta \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.fasta\" \\ --germline_resource \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/gnomAD.r2.1.1.vcf.gz\" \\ --intervals \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.interval_list\" \\ --known_indels \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz\" \\ --snpeff_db 105 \\ --snpeff_genome \"WBcel235\" \\ --snpeff_version \"5.1\" \\ --vep_cache_version \"106\" \\ --vep_genome \"WBcel235\" \\ --vep_species \"caenorhabditis_elegans\" \\ --vep_version \"106.1\" \\ --max_cpus 2 \\ --max_memory 6.5GB \\ --tools \"freebayes\" \\ --outdir \"my_results\" Tools The --tools parameter is included above to trigger the execution of the freebayes variant caller. Multiple variant callers are available as a part of Sarek, however, in this example, only one is included. Warning The --igenomes_ignore parameter must be included when using custom reference/annotation files. Without it, by default, the reference/annotation files that are typically required by Sarek are downloaded for GATK.GRCh38 . Some pipelines have this feature by default while others require the genome flag (or alternate) for every execution.","title":"Reference data (--genome)"},{"location":"session_2/4_runConfiguration/#profiles-profile","text":"Software profiles are used to specify the software environment in which the pipeline will be executed. By simply including a profile (e.g., singularity ), Nextflow will download, store, and manage the software used in the Sarek pipeline. To ensure reproducibility, it is recommended that you use container technology, e.g., docker or singularity .","title":"Profiles (--profile)"},{"location":"session_2/4_runConfiguration/#putting-it-all-together","text":"The previous sections have shown different parts of a recommended run command. These can be combined to create a custom run command that will execute Sarek on the small test data files. The completed run command will execute a small test set of files using the freebayes variant caller. The command will use the small test data files from the nf-core test datasets repository and custom reference/annotation files that are hosted on github. Exercise Use all of the information above to build a custom run command that will execute version 3.2.3 of Sarek on the samplesheet you created. Hint Include the sample sheet you made earlier with --input samplesheet.csv . Remember is must be in your working directory or you must specify the full or relative path to the file. If you named your samplesheet differently it must be reflected in your command. Include the parameters shown above to specify the reference/annotation files. You can copy these directly into your run command. Use Singularity to manage your software with -profile singularity . Solution Your run command should look like the following: nextflow run nf-core/sarek \\ --input samplesheet.csv \\ --igenomes_ignore \\ --dbsnp \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz\" \\ --fasta \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.fasta\" \\ --germline_resource \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/gnomAD.r2.1.1.vcf.gz\" \\ --intervals \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.interval_list\" \\ --known_indels \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz\" \\ --snpeff_db 105 \\ --snpeff_genome \"WBcel235\" \\ --snpeff_version \"5.1\" \\ --vep_cache_version \"106\" \\ --vep_genome \"WBcel235\" \\ --vep_species \"caenorhabditis_elegans\" \\ --vep_version \"106.1\" \\ --max_cpus 2 \\ --max_memory 6 .5GB \\ --tools \"freebayes\" \\ --outdir \"my_results\" \\ -profile singularity \\ -r 3 .2.3 If everything has worked - you will see the pipeline launching in your terminal \ud83d\ude80 Key points Sarek comes with a test profiles that can be used to test the pipeline on your infrastructure Sample sheets are csv files that contain important meta data and the paths to your files Reference files are available from iGenomes","title":"Putting it all together"},{"location":"session_3/0_homeStretchOverview/","text":"Session three overview \u00b6 Session three expands on configuring your pipeline run. You will take the run command you built in the previous session and customize various parameters.","title":"Session three overview"},{"location":"session_3/0_homeStretchOverview/#session-three-overview","text":"Session three expands on configuring your pipeline run. You will take the run command you built in the previous session and customize various parameters.","title":"Session three overview"},{"location":"session_3/1_configFiles/","text":"Configuration basics \u00b6 Configuring nf-core pipelines \u00b6 Objectives Learn how to customize the execution of an nf-core pipeline. Customize a toy example of an nf-core pipeline. Configuration \u00b6 Each nf-core pipeline comes with a set of \u201csensible defaults\u201d. While the defaults are a great place to start, you will almost certainly want to modify these to fit your own purposes and system requirements. You do not need to edit the pipeline code to configure nf-core pipelines. When a pipeline is launched, Nextflow will look for configuration files in several locations. As each source can contain conflicting settings, the sources are ranked to decide which settings to apply. Configuration sources are reported below and listed in order of priority: Parameters specified on the command line ( --parameter ) Parameters that are provided using the -params-file option Config file that are provided using the -c option The config file named nextflow.config in the current directory The config file named nextflow.config in the pipeline project directory The config file $HOME/.nextflow/config Values defined within the pipeline script itself (e.g., main.nf ) Warning nf-core pipeline parameters must be passed via the command line ( --<parameter> ) or Nextflow -params-file option. Custom config files, including those provided by the -c option, can be used to provide any configuration except for parameters. Notably, while some of these files are already included in the nf-core pipeline repository (e.g., the nextflow.config file in the nf-core pipeline repository), some are automatically identified on your local system (e.g., the nextflow.config in the launch directory), and others are only included if they are specified using run options (e.g., -params-file , and -c ). Understanding how and when these files are interpreted by Nextflow is critical for the accurate configuration of a pipelines execution. Parameters \u00b6 Parameters are pipeline specific settings that can be used to customise the execution of a pipeline. Every nf-core pipeline has a full list of parameters on the nf-core website. When viewing these parameters online, you will also be shown a description and the type of the parameter. Some parameters will have additional text to help you understand when and how a parameter should be used. Parameters and their descriptions can also be viewed in the command line using the run command with the --help parameter: nextflow run nf-core/<workflow> --help Exercise View the parameters for the christopher-hakkaart/nf-core-demo pipeline using the command line: Solution The christopher-hakkaart/nf-core-demo pipeline parameters can be printed using the run command and the --help option: nextflow run christopher-hakkaart/nf-core-demo -r main --help Parameters in the command line \u00b6 At the highest level, parameters can be customised using the command line. Any parameter can be configured on the command line by prefixing the parameter name with a double dash ( -- ): nextflow nf-core/<workflow> --<parameter> When to use -- and - Nextflow options are prefixed with a single dash ( - ) and pipeline parameters are prefixed with a double dash ( -- ). Depending on the parameter type, you may be required to add additional information after your parameter flag. For example, for a string parameter, you would add the string after the parameter flag: nextflow nf-core/<workflow> --<parameter> string Exercise Give the MultiQC report for the christopher-hakkaart/nf-core-demo pipeline the name of your favorite animal using the multiqc_title parameter using a command line flag: Solution Add the --multiqc_title flag to your command and execute it. Use the -resume option to save time: nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main --multiqc_title kiwi -resume In this example, you can check your parameter has been applied by listing the files created in the results folder ( results ): ls results/multiqc/ --multiqc_title is a parameter that directly impacts a result file. For parameters that are not as obvious, you may need to check your log to ensure your changes have been applied. You should not rely on the changes to parameters printed to the command line when you execute your run: nextflow log nextflow log <run name> -f \"process,script\" Custom configuration files \u00b6 Nextflow will also look for files that are external to the pipeline project directory. These files include: The config file $HOME/.nextflow/config A config file named nextflow.config in your current directory Custom files specified using the command line A parameter file that is provided using the -params-file option A config file that are provided using the -c option You don't need to use all of these files to execute your pipeline. ** Parameter files Parameter files are .json files that can contain an unlimited number of parameters: my-params.json { \"<parameter1_name>\" : 1 , \"<parameter2_name>\" : \"<string>\" , \"<parameter3_name>\" : true } You can override default parameters by creating a custom .json file and passing it as a command-line argument using the -param-file option. nextflow run nf-core/<workflow> -profile test,singularity -r main -param-file <path/to/params.json> Customizing parameters \u00b6 Let's take the skills from the previous section and apply them to customise the execution of the Sarek pipeline. Remember that previoiusly we supplied a series of Sarek pipeline parameters as flags in your run command ( -- ). Here, we will package these into a .json file and use the -params-file option. Exercise Package the parameters from the previous lesson into a .json file and run the pipeline using the -params-file option: nextflow run nf-core/sarek \\ --input samplesheet.csv \\ --igenomes_ignore \\ --dbsnp \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz\" \\ --fasta \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.fasta\" \\ --germline_resource \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/gnomAD.r2.1.1.vcf.gz\" \\ --intervals \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.interval_list\" \\ --known_indels \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz\" \\ --snpeff_db 105 \\ --snpeff_genome \"WBcel235\" \\ --snpeff_version \"5.1\" \\ --tools \"freebayes\" \\ --vep_cache_version \"106\" \\ --vep_genome \"WBcel235\" \\ --vep_species \"caenorhabditis_elegans\" \\ --vep_version \"106.1\" \\ --max_cpus 4 \\ --max_memory 6.5GB \\ --output \"my_results\" -profile singularity \\ -r 3.2.3 Solution my-params.json { \"igenomes_ignore\" : true , \"dbsnp\" : \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz\" , \"fasta\" : \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.fasta\" , \"germline_resource\" : \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/gnomAD.r2.1.1.vcf.gz\" , \"intervals\" : \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.interval_list\" , \"known_indels\" : \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz\" , \"snpeff_db\" : 105 , \"snpeff_genome\" : \"WBcel235\" , \"snpeff_version\" : \"5.1\" , \"tools\" : \"freebayes\" , \"vep_cache_version\" : 106 , \"vep_genome\" : \"WBcel235\" , \"vep_species\" : \"caenorhabditis_elegans\" , \"vep_version\" : \"106.1\" , \"max_cpus\" : 4 , \"max_memory\" : \"6.5 GB\" , \"outdir\" : \"my_results_2\" } Your execution command will now look like this: nextflow run nf-core/sarek --input samplesheet.csv -params-file my-params.json -profile singularity -r 3 .2.3 Note that in this example we kept --input samplesheet.csv in the execution command. However, this could have put this in the .json file. You can pick and choose which parameters go in a params file and which parameters go in your execution command. Due to the order of priority, you can modify parameters you want to change without having to edit your newly created parameters file. Exercise Include both freebayes and strelka as variant callers using the tools parameter and run the pipeline again. For this option, you will need to use the --tools flag and include both variant callers in the same string separated by a comma, e.g., --tools \"<tool1>,<tool2>\" You can also use -resume to resume the pipeline from the last successful step. Solution nextflow run nf-core/sarek --input samplesheet.csv -params-file my-params.json -profile singularity -r 3 .2.3 --tools \"freebayes,strelka\" -resume Default configuration files \u00b6 All parameters will have a default setting that is defined using the nextflow.config file in the pipeline project directory. By default, most parameters are set to null or false and are only activated by a profile or configuration file. There are also several includeConfig statements in the nextflow.config file that are used to include additional .config files from the conf/ folder. Each additional .config file contains categorised configuration information for your pipeline execution, some of which can be optionally included: base.config Included by the pipeline by default. Generous resource allocations using labels. Does not specify any method for software management and expects software to be available (or specified elsewhere). igenomes.config Included by the pipeline by default. Default configuration to access reference files stored on AWS iGenomes . modules.config Included by the pipeline by default. Module-specific configuration options (both mandatory and optional). test.config Only included if specified as a profile. A configuration profile to test the pipeline with a small test dataset. test_full.config Only included if specified as a profile. A configuration profile to test the pipeline with a full-size test dataset. Notably, configuration files can also contain the definition of one or more profiles. A profile is a set of configuration attributes that can be activated when launching a pipeline by using the -profile command option: nextflow run nf-core/<workflow> -profile <profile> Profiles used by nf-core pipelines include: Software management profiles Profiles for the management of software using software management tools, e.g., docker , singularity , and conda . Test profiles Profiles to execute the pipeline with a standardised set of test data and parameters, e.g., test and test_full . Multiple profiles can be specified in a comma-separated ( , ) list when you execute your command. The order of profiles is important as they will be read from left to right: nextflow run nf-core/<workflow> -profile test,singularity nf-core pipelines are required to define software containers and conda environments that can be activated using profiles. Although it is possible to run the pipelines with software installed by other methods (e.g., environment modules or manual installation), using Docker or Singularity is more convenient and more reproducible. Tip If you're computer has internet access and one of Conda, Singularity, or Docker installed, you should be able to run any nf-core pipeline with the test profile and the respective software management profile 'out of the box'. The test data profile will pull small test files directly from the nf-core/test-data GitHub repository and run it on your local system. The test profile is an important control to check the pipeline is working as expected and is a great way to trial a pipeline. Some pipelines have multiple test profiles for you to try. Key points nf-core pipelines follow a similar structure. nf-core pipelines are configured using multiple configuration sources. Configuration sources are ranked to decide which settings to apply. Pipeline parameters must be passed via the command line ( --<parameter> ) or Nextflow -params-file option.","title":"Configuration basics"},{"location":"session_3/1_configFiles/#configuration-basics","text":"","title":"Configuration basics"},{"location":"session_3/1_configFiles/#configuring-nf-core-pipelines","text":"Objectives Learn how to customize the execution of an nf-core pipeline. Customize a toy example of an nf-core pipeline.","title":"Configuring nf-core pipelines"},{"location":"session_3/1_configFiles/#configuration","text":"Each nf-core pipeline comes with a set of \u201csensible defaults\u201d. While the defaults are a great place to start, you will almost certainly want to modify these to fit your own purposes and system requirements. You do not need to edit the pipeline code to configure nf-core pipelines. When a pipeline is launched, Nextflow will look for configuration files in several locations. As each source can contain conflicting settings, the sources are ranked to decide which settings to apply. Configuration sources are reported below and listed in order of priority: Parameters specified on the command line ( --parameter ) Parameters that are provided using the -params-file option Config file that are provided using the -c option The config file named nextflow.config in the current directory The config file named nextflow.config in the pipeline project directory The config file $HOME/.nextflow/config Values defined within the pipeline script itself (e.g., main.nf ) Warning nf-core pipeline parameters must be passed via the command line ( --<parameter> ) or Nextflow -params-file option. Custom config files, including those provided by the -c option, can be used to provide any configuration except for parameters. Notably, while some of these files are already included in the nf-core pipeline repository (e.g., the nextflow.config file in the nf-core pipeline repository), some are automatically identified on your local system (e.g., the nextflow.config in the launch directory), and others are only included if they are specified using run options (e.g., -params-file , and -c ). Understanding how and when these files are interpreted by Nextflow is critical for the accurate configuration of a pipelines execution.","title":"Configuration"},{"location":"session_3/1_configFiles/#parameters","text":"Parameters are pipeline specific settings that can be used to customise the execution of a pipeline. Every nf-core pipeline has a full list of parameters on the nf-core website. When viewing these parameters online, you will also be shown a description and the type of the parameter. Some parameters will have additional text to help you understand when and how a parameter should be used. Parameters and their descriptions can also be viewed in the command line using the run command with the --help parameter: nextflow run nf-core/<workflow> --help Exercise View the parameters for the christopher-hakkaart/nf-core-demo pipeline using the command line: Solution The christopher-hakkaart/nf-core-demo pipeline parameters can be printed using the run command and the --help option: nextflow run christopher-hakkaart/nf-core-demo -r main --help","title":"Parameters"},{"location":"session_3/1_configFiles/#parameters-in-the-command-line","text":"At the highest level, parameters can be customised using the command line. Any parameter can be configured on the command line by prefixing the parameter name with a double dash ( -- ): nextflow nf-core/<workflow> --<parameter> When to use -- and - Nextflow options are prefixed with a single dash ( - ) and pipeline parameters are prefixed with a double dash ( -- ). Depending on the parameter type, you may be required to add additional information after your parameter flag. For example, for a string parameter, you would add the string after the parameter flag: nextflow nf-core/<workflow> --<parameter> string Exercise Give the MultiQC report for the christopher-hakkaart/nf-core-demo pipeline the name of your favorite animal using the multiqc_title parameter using a command line flag: Solution Add the --multiqc_title flag to your command and execute it. Use the -resume option to save time: nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main --multiqc_title kiwi -resume In this example, you can check your parameter has been applied by listing the files created in the results folder ( results ): ls results/multiqc/ --multiqc_title is a parameter that directly impacts a result file. For parameters that are not as obvious, you may need to check your log to ensure your changes have been applied. You should not rely on the changes to parameters printed to the command line when you execute your run: nextflow log nextflow log <run name> -f \"process,script\"","title":"Parameters in the command line"},{"location":"session_3/1_configFiles/#custom-configuration-files","text":"Nextflow will also look for files that are external to the pipeline project directory. These files include: The config file $HOME/.nextflow/config A config file named nextflow.config in your current directory Custom files specified using the command line A parameter file that is provided using the -params-file option A config file that are provided using the -c option You don't need to use all of these files to execute your pipeline. ** Parameter files Parameter files are .json files that can contain an unlimited number of parameters: my-params.json { \"<parameter1_name>\" : 1 , \"<parameter2_name>\" : \"<string>\" , \"<parameter3_name>\" : true } You can override default parameters by creating a custom .json file and passing it as a command-line argument using the -param-file option. nextflow run nf-core/<workflow> -profile test,singularity -r main -param-file <path/to/params.json>","title":"Custom configuration files"},{"location":"session_3/1_configFiles/#customizing-parameters","text":"Let's take the skills from the previous section and apply them to customise the execution of the Sarek pipeline. Remember that previoiusly we supplied a series of Sarek pipeline parameters as flags in your run command ( -- ). Here, we will package these into a .json file and use the -params-file option. Exercise Package the parameters from the previous lesson into a .json file and run the pipeline using the -params-file option: nextflow run nf-core/sarek \\ --input samplesheet.csv \\ --igenomes_ignore \\ --dbsnp \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz\" \\ --fasta \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.fasta\" \\ --germline_resource \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/gnomAD.r2.1.1.vcf.gz\" \\ --intervals \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.interval_list\" \\ --known_indels \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz\" \\ --snpeff_db 105 \\ --snpeff_genome \"WBcel235\" \\ --snpeff_version \"5.1\" \\ --tools \"freebayes\" \\ --vep_cache_version \"106\" \\ --vep_genome \"WBcel235\" \\ --vep_species \"caenorhabditis_elegans\" \\ --vep_version \"106.1\" \\ --max_cpus 4 \\ --max_memory 6.5GB \\ --output \"my_results\" -profile singularity \\ -r 3.2.3 Solution my-params.json { \"igenomes_ignore\" : true , \"dbsnp\" : \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/dbsnp_146.hg38.vcf.gz\" , \"fasta\" : \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.fasta\" , \"germline_resource\" : \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/gnomAD.r2.1.1.vcf.gz\" , \"intervals\" : \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/genome.interval_list\" , \"known_indels\" : \"https://raw.githubusercontent.com/nf-core/test-datasets/modules/data/genomics/homo_sapiens/genome/vcf/mills_and_1000G.indels.vcf.gz\" , \"snpeff_db\" : 105 , \"snpeff_genome\" : \"WBcel235\" , \"snpeff_version\" : \"5.1\" , \"tools\" : \"freebayes\" , \"vep_cache_version\" : 106 , \"vep_genome\" : \"WBcel235\" , \"vep_species\" : \"caenorhabditis_elegans\" , \"vep_version\" : \"106.1\" , \"max_cpus\" : 4 , \"max_memory\" : \"6.5 GB\" , \"outdir\" : \"my_results_2\" } Your execution command will now look like this: nextflow run nf-core/sarek --input samplesheet.csv -params-file my-params.json -profile singularity -r 3 .2.3 Note that in this example we kept --input samplesheet.csv in the execution command. However, this could have put this in the .json file. You can pick and choose which parameters go in a params file and which parameters go in your execution command. Due to the order of priority, you can modify parameters you want to change without having to edit your newly created parameters file. Exercise Include both freebayes and strelka as variant callers using the tools parameter and run the pipeline again. For this option, you will need to use the --tools flag and include both variant callers in the same string separated by a comma, e.g., --tools \"<tool1>,<tool2>\" You can also use -resume to resume the pipeline from the last successful step. Solution nextflow run nf-core/sarek --input samplesheet.csv -params-file my-params.json -profile singularity -r 3 .2.3 --tools \"freebayes,strelka\" -resume","title":"Customizing parameters"},{"location":"session_3/1_configFiles/#default-configuration-files","text":"All parameters will have a default setting that is defined using the nextflow.config file in the pipeline project directory. By default, most parameters are set to null or false and are only activated by a profile or configuration file. There are also several includeConfig statements in the nextflow.config file that are used to include additional .config files from the conf/ folder. Each additional .config file contains categorised configuration information for your pipeline execution, some of which can be optionally included: base.config Included by the pipeline by default. Generous resource allocations using labels. Does not specify any method for software management and expects software to be available (or specified elsewhere). igenomes.config Included by the pipeline by default. Default configuration to access reference files stored on AWS iGenomes . modules.config Included by the pipeline by default. Module-specific configuration options (both mandatory and optional). test.config Only included if specified as a profile. A configuration profile to test the pipeline with a small test dataset. test_full.config Only included if specified as a profile. A configuration profile to test the pipeline with a full-size test dataset. Notably, configuration files can also contain the definition of one or more profiles. A profile is a set of configuration attributes that can be activated when launching a pipeline by using the -profile command option: nextflow run nf-core/<workflow> -profile <profile> Profiles used by nf-core pipelines include: Software management profiles Profiles for the management of software using software management tools, e.g., docker , singularity , and conda . Test profiles Profiles to execute the pipeline with a standardised set of test data and parameters, e.g., test and test_full . Multiple profiles can be specified in a comma-separated ( , ) list when you execute your command. The order of profiles is important as they will be read from left to right: nextflow run nf-core/<workflow> -profile test,singularity nf-core pipelines are required to define software containers and conda environments that can be activated using profiles. Although it is possible to run the pipelines with software installed by other methods (e.g., environment modules or manual installation), using Docker or Singularity is more convenient and more reproducible. Tip If you're computer has internet access and one of Conda, Singularity, or Docker installed, you should be able to run any nf-core pipeline with the test profile and the respective software management profile 'out of the box'. The test data profile will pull small test files directly from the nf-core/test-data GitHub repository and run it on your local system. The test profile is an important control to check the pipeline is working as expected and is a great way to trial a pipeline. Some pipelines have multiple test profiles for you to try. Key points nf-core pipelines follow a similar structure. nf-core pipelines are configured using multiple configuration sources. Configuration sources are ranked to decide which settings to apply. Pipeline parameters must be passed via the command line ( --<parameter> ) or Nextflow -params-file option.","title":"Default configuration files"},{"location":"session_3/2_configuration/","text":"Metrics and reports \u00b6 Nextflow can also produce multiple reports and charts that show several runtime metrics and your execution information. You can enable this functionality by adding Nextflow options to your run command: Adding -with-report to your run command will create a HTML execution report which includes many useful metrics about a pipeline execution. Adding -with-trace option to creates an execution tracing file that contains some useful information about each process executed in your pipeline script. Adding -with-timeline to your run command enables the creation of the pipeline timeline report showing how processes were executed over time. Adding -with-dag to your run command enables the rendering of the pipeline execution direct acyclic graph representation. This feature requires the installation of Graphviz on your computer. Beginning in version 22.04, Nextflow can render the DAG as a Mermaid diagram. Mermaid diagrams are particularly useful because they can be embedded in GitHub Flavored Markdown without having to render them yourself. Note The execution report ( -with-report ), trace report ( -with-trace ), timeline trace ( -with-timeline ), and dag ( -with-dag ) must be specified when the pipeline is executed. By contrast, the log option is useful after a pipeline has already run and is available for every executed pipeline. Exercise Try to run the following command and view the reports generated by Nextflow: nextflow run nf-core/sarek --input samplesheet.csv -params-file my-params.json -profile singularity -r 3 .2.3 --tools \"freebayes,strelka\" -with-report -with-trace -with-timeline -with-dag -resume Key points Parameters go in parameters files and everything else goes in a configuration file There are additional flags you can use to generate reports and metric for you own records","title":"Metrics and reports"},{"location":"session_3/2_configuration/#metrics-and-reports","text":"Nextflow can also produce multiple reports and charts that show several runtime metrics and your execution information. You can enable this functionality by adding Nextflow options to your run command: Adding -with-report to your run command will create a HTML execution report which includes many useful metrics about a pipeline execution. Adding -with-trace option to creates an execution tracing file that contains some useful information about each process executed in your pipeline script. Adding -with-timeline to your run command enables the creation of the pipeline timeline report showing how processes were executed over time. Adding -with-dag to your run command enables the rendering of the pipeline execution direct acyclic graph representation. This feature requires the installation of Graphviz on your computer. Beginning in version 22.04, Nextflow can render the DAG as a Mermaid diagram. Mermaid diagrams are particularly useful because they can be embedded in GitHub Flavored Markdown without having to render them yourself. Note The execution report ( -with-report ), trace report ( -with-trace ), timeline trace ( -with-timeline ), and dag ( -with-dag ) must be specified when the pipeline is executed. By contrast, the log option is useful after a pipeline has already run and is available for every executed pipeline. Exercise Try to run the following command and view the reports generated by Nextflow: nextflow run nf-core/sarek --input samplesheet.csv -params-file my-params.json -profile singularity -r 3 .2.3 --tools \"freebayes,strelka\" -with-report -with-trace -with-timeline -with-dag -resume Key points Parameters go in parameters files and everything else goes in a configuration file There are additional flags you can use to generate reports and metric for you own records","title":"Metrics and reports"},{"location":"session_3/3_appendix/","text":"Appendix \u00b6 Appendix 1 Shared configuration files \u00b6 An includeConfig statement in the nextflow.config file is also used to include custom institutional profiles that have been submitted to the nf-core config repository . At run time, nf-core pipelines will fetch these configuration profiles from the nf-core config repository and make them available. For shared resources such as an HPC cluster, you may consider developing a shared institutional profile. You can follow this tutorial for more help. Appendix 2 Custom config file exercise \u00b6 Exercise Give the MultiQC report for the christopher-hakkaart/nf-core-demo pipeline the name of your favorite food using the multiqc_title parameter in a parameters file: Solution Create a custom .json file that contains your favourite food, e.g., cheese: my-custom-params.json { \"multiqc_title\" : \"cheese\" } Include the custom .json file in your execution command with the -params-file option: nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main -params-file my_custom_params.json Check that it has been applied: ls results/multiqc/ Appendix 3 Configuration files \u00b6 Configuration files are .config files that can contain various pipeline properties. Custom paths passed in the command-line using the -c option: nextflow run nf-core/<workflow> -profile test,singularity -c <path/to/custom.config> Multiple custom .config files can be included at execution by separating them with a comma ( , ). Custom configuration files follow the same structure as the configuration file included in the pipeline directory. Configuration properties are organised into scopes by dot prefixing the property names with a scope identifier or grouping the properties in the same scope using the curly brackets notation. For example: custom.config alpha.x = 1 alpha.y = 'string value' Is equivalent to: custom.config alpha { x = 1 y = 'string value' } Scopes allow you to quickly configure settings required to deploy a pipeline on different infrastructure using different software management. For example, the executor scope can be used to provide settings for the deployment of a pipeline on a HPC cluster. Similarly, the singularity scope controls how Singularity containers are executed by Nextflow. Multiple scopes can be included in the same .config file using a mix of dot prefixes and curly brackets. A full list of scopes is described in detail here . Exercise Give the MultiQC report for the christopher-hakkaart/nf-core-demo pipeline the name of your favorite color using the multiqc_title parameter in a custom .config file: Solution Create a custom .config file that contains your favourite colour, e.g., blue: custom.config params.multiqc_title = \"blue\" Include the custom .config file in your execution command with the -c option: nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main -resume -c custom.config Check that it has been applied: ls results/multiqc/ Why did this fail? You can not use the params scope in custom configuration files. Parameters can only be configured using the -params-file option and the command line. While it parameter is listed as a parameter on the STDOUT , it was not applied to the executed command: nextflow log nextflow log <run name> -f \"process,script\" The process scope allows you to configure pipeline processes and is used extensively to define resources and additional arguments for modules. By default, process resources are allocated in the conf/base.config file using the withLabel selector: base.config process { withLabel: BIG_JOB { cpus = 16 memory = 64 .GB } } Similarly, the withName selector enables the configuration of a process by name. By default, module parameters are defined in the conf/modules.config file: modules.config process { withName: MYPROCESS { cpus = 4 memory = 8 .GB } } While some tool arguments are included as a part of a module. To make modules sharable across pipelines, most tool arguments are defined in the conf/modules.conf file in the pipeline code under the ext.args entry. Importantly, having these arguments outside of the module also allows them to be customised at runtime. For example, if you were trying to add arguments in the MULTIQC process in the christopher-hakkaart/nf-core-demo pipeline, you could use the process scope: custom.config process { withName : \".*:MULTIQC\" { ext.args = { \"<your custom parameter>\" } } However, if a process is used multiple times in the same pipeline, an extended execution path of the module may be required to make it more specific: custom.config process { withName: \"NFCORE_DEMO:DEMO:MULTIQC\" { ext.args = \"<your custom parameter>\" } } The extended execution path is built from the pipelines, subworkflows, and module used to execute the process. In the example above, the nf-core MULTIQC module, was called by the DEMO pipeline, which was called by the NFCORE_DEMO pipeline in the main.nf file. How to build an extended execution path It can be tricky to evaluate the path used to execute a module. If you are unsure of how to build the path you can copy it from the conf/modules.conf file. How arguments are added to a process can also vary. Be vigilant. Exercise Create a new .config file that uses the process scope to overwrite the args for the MULTIQC process. Change the args to your favourite month of the year, e.g, \"--title \\\"october\\\"\" . In this example, the \\ is used to escape the \" in the string. This is required to ensure the string is passed correctly to the MULTIQC module. Solution Make a custom config file that uses the process scope to replace the args for the MULTIQC process: custom.config process { withName: \"NFCORE_DEMO:DEMO:MULTIQC\" { ext.args = \"--title \\\"october\\\"\" } } Execute your run command again with the custom configuration file: nextflow run christopher-hakkaart/nf-core-demo -r main -profile test,singularity -resume -c custom.config Check that it has been applied: ls results/multiqc/ Exercise Demonstrate the configuration hierarchy using the christopher-hakkaart/nf-core-demo pipeline by adding a params file ( -params-file ), and a command line flag ( --multiqc_title ) to your execution. You can use the files you have already created. Make sure that the --multiqc_title is different to the multiqc_title in your params file and different to the title you have used in the past. Solution Use the .json file you created previously: my-custom-params.json { \"multiqc_title\" : \"cheese\" } Execute your command with your params file ( -params-file ) and a command line flag ( --multiqc_title ): nextflow run christopher-hakkaart/nf-core-demo -r main -profile test,singularity -resume -params-file my_custom_params.json --multiqc_title \"cake\" In this example, as the command line is at the top of the hierarchy, the multiqc_title will be \"cake\". Configuration files \u00b6 Sometimes Sarek won't have a parameter that you need to customize the execution of a tool. In this situation, you will need to apply a configuration file to the pipeline. As shown in the example from Session 1 , you can selectively apply a configuration file to a process using the withName directive. Be specific with your selector Remember to make your selector specific to the process you are trying to customise. It can be helpful to use the same selectors that are already included in the configuration file. Sarek is a little different from other pipelines because it has multiple different config files that are used for different tools or groups of tools. This is stylistic and helps to keep the config files organised. For example, there are a number of options that are applied when calling variants with FREEBAYES and are all stored in a freebayes.config file together: freebayes.config process { withName: 'MERGE_FREEBAYES' { ext.prefix = { \"${meta.id}.freebayes\" } publishDir = [ mode: params.publish_dir_mode, path: { \"${params.outdir}/variant_calling/freebayes/${meta.id}/\" }, saveAs: { filename -> filename.equals('versions.yml') ? null : filename } ] } withName: 'FREEBAYES' { ext.args = '--min-alternate-fraction 0.1 --min-mapping-quality 1' //To make sure no naming conflicts ensure with module BCFTOOLS_SORT & the naming being correct in the output folder ext.prefix = { meta.num_intervals <= 1 ? \"${meta.id}\" : \"${meta.id}.${target_bed.simpleName}\" } ext.when = { params.tools && params.tools.split(',').contains('freebayes') } publishDir = [ enabled: false ] } withName: 'BCFTOOLS_SORT' { ext.prefix = { meta.num_intervals <= 1 ? meta.id + \".freebayes\" : vcf.name - \".vcf\" + \".sort\" } publishDir = [ mode: params.publish_dir_mode, path: { \"${params.outdir}/variant_calling/\" }, pattern: \"*vcf.gz\", saveAs: { meta.num_intervals > 1 ? null : \"freebayes/${meta.id}/${it}\" } ] } withName : 'TABIX_VC_FREEBAYES' { publishDir = [ mode: params.publish_dir_mode, path: { \"${params.outdir}/variant_calling/freebayes/${meta.id}/\" }, saveAs: { filename -> filename.equals('versions.yml') ? null : filename } ] } // PAIR_VARIANT_CALLING if (params.tools && params.tools.split(',').contains('freebayes')) { withName: '.*:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_FREEBAYES:FREEBAYES' { ext.args = \"--pooled-continuous \\ --pooled-discrete \\ --genotype-qualities \\ --report-genotype-likelihood-max \\ --allele-balance-priors-off \\ --min-alternate-fraction 0.03 \\ --min-repeat-entropy 1 \\ --min-alternate-count 2 \" } } } Each of these can be modified independently of the others and be applied using a custom configuration file. Exercise Create a custom configuration file that will modify the --min-alternate-fraction parameter for FREEBAYES to 0.05 and apply it to the pipeline. Solution nextflow run nf-core/sarek --input samplesheet.csv -params-file my-params.json -profile singularity -r 3 .2.3 --tools \"freebayes,strelka\" -resume -c my-config.config my-config.config process { withName: 'FREEBAYES' { ext.args = '--min-alternate-fraction 0.05' } }","title":"Appendix"},{"location":"session_3/3_appendix/#appendix","text":"","title":"Appendix"},{"location":"session_3/3_appendix/#appendix-1-shared-configuration-files","text":"An includeConfig statement in the nextflow.config file is also used to include custom institutional profiles that have been submitted to the nf-core config repository . At run time, nf-core pipelines will fetch these configuration profiles from the nf-core config repository and make them available. For shared resources such as an HPC cluster, you may consider developing a shared institutional profile. You can follow this tutorial for more help.","title":"Appendix 1 Shared configuration files"},{"location":"session_3/3_appendix/#appendix-2-custom-config-file-exercise","text":"Exercise Give the MultiQC report for the christopher-hakkaart/nf-core-demo pipeline the name of your favorite food using the multiqc_title parameter in a parameters file: Solution Create a custom .json file that contains your favourite food, e.g., cheese: my-custom-params.json { \"multiqc_title\" : \"cheese\" } Include the custom .json file in your execution command with the -params-file option: nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main -params-file my_custom_params.json Check that it has been applied: ls results/multiqc/","title":"Appendix 2 Custom config file exercise"},{"location":"session_3/3_appendix/#appendix-3-configuration-files","text":"Configuration files are .config files that can contain various pipeline properties. Custom paths passed in the command-line using the -c option: nextflow run nf-core/<workflow> -profile test,singularity -c <path/to/custom.config> Multiple custom .config files can be included at execution by separating them with a comma ( , ). Custom configuration files follow the same structure as the configuration file included in the pipeline directory. Configuration properties are organised into scopes by dot prefixing the property names with a scope identifier or grouping the properties in the same scope using the curly brackets notation. For example: custom.config alpha.x = 1 alpha.y = 'string value' Is equivalent to: custom.config alpha { x = 1 y = 'string value' } Scopes allow you to quickly configure settings required to deploy a pipeline on different infrastructure using different software management. For example, the executor scope can be used to provide settings for the deployment of a pipeline on a HPC cluster. Similarly, the singularity scope controls how Singularity containers are executed by Nextflow. Multiple scopes can be included in the same .config file using a mix of dot prefixes and curly brackets. A full list of scopes is described in detail here . Exercise Give the MultiQC report for the christopher-hakkaart/nf-core-demo pipeline the name of your favorite color using the multiqc_title parameter in a custom .config file: Solution Create a custom .config file that contains your favourite colour, e.g., blue: custom.config params.multiqc_title = \"blue\" Include the custom .config file in your execution command with the -c option: nextflow run christopher-hakkaart/nf-core-demo -profile test,singularity -r main -resume -c custom.config Check that it has been applied: ls results/multiqc/ Why did this fail? You can not use the params scope in custom configuration files. Parameters can only be configured using the -params-file option and the command line. While it parameter is listed as a parameter on the STDOUT , it was not applied to the executed command: nextflow log nextflow log <run name> -f \"process,script\" The process scope allows you to configure pipeline processes and is used extensively to define resources and additional arguments for modules. By default, process resources are allocated in the conf/base.config file using the withLabel selector: base.config process { withLabel: BIG_JOB { cpus = 16 memory = 64 .GB } } Similarly, the withName selector enables the configuration of a process by name. By default, module parameters are defined in the conf/modules.config file: modules.config process { withName: MYPROCESS { cpus = 4 memory = 8 .GB } } While some tool arguments are included as a part of a module. To make modules sharable across pipelines, most tool arguments are defined in the conf/modules.conf file in the pipeline code under the ext.args entry. Importantly, having these arguments outside of the module also allows them to be customised at runtime. For example, if you were trying to add arguments in the MULTIQC process in the christopher-hakkaart/nf-core-demo pipeline, you could use the process scope: custom.config process { withName : \".*:MULTIQC\" { ext.args = { \"<your custom parameter>\" } } However, if a process is used multiple times in the same pipeline, an extended execution path of the module may be required to make it more specific: custom.config process { withName: \"NFCORE_DEMO:DEMO:MULTIQC\" { ext.args = \"<your custom parameter>\" } } The extended execution path is built from the pipelines, subworkflows, and module used to execute the process. In the example above, the nf-core MULTIQC module, was called by the DEMO pipeline, which was called by the NFCORE_DEMO pipeline in the main.nf file. How to build an extended execution path It can be tricky to evaluate the path used to execute a module. If you are unsure of how to build the path you can copy it from the conf/modules.conf file. How arguments are added to a process can also vary. Be vigilant. Exercise Create a new .config file that uses the process scope to overwrite the args for the MULTIQC process. Change the args to your favourite month of the year, e.g, \"--title \\\"october\\\"\" . In this example, the \\ is used to escape the \" in the string. This is required to ensure the string is passed correctly to the MULTIQC module. Solution Make a custom config file that uses the process scope to replace the args for the MULTIQC process: custom.config process { withName: \"NFCORE_DEMO:DEMO:MULTIQC\" { ext.args = \"--title \\\"october\\\"\" } } Execute your run command again with the custom configuration file: nextflow run christopher-hakkaart/nf-core-demo -r main -profile test,singularity -resume -c custom.config Check that it has been applied: ls results/multiqc/ Exercise Demonstrate the configuration hierarchy using the christopher-hakkaart/nf-core-demo pipeline by adding a params file ( -params-file ), and a command line flag ( --multiqc_title ) to your execution. You can use the files you have already created. Make sure that the --multiqc_title is different to the multiqc_title in your params file and different to the title you have used in the past. Solution Use the .json file you created previously: my-custom-params.json { \"multiqc_title\" : \"cheese\" } Execute your command with your params file ( -params-file ) and a command line flag ( --multiqc_title ): nextflow run christopher-hakkaart/nf-core-demo -r main -profile test,singularity -resume -params-file my_custom_params.json --multiqc_title \"cake\" In this example, as the command line is at the top of the hierarchy, the multiqc_title will be \"cake\".","title":"Appendix 3 Configuration files"},{"location":"session_3/3_appendix/#configuration-files","text":"Sometimes Sarek won't have a parameter that you need to customize the execution of a tool. In this situation, you will need to apply a configuration file to the pipeline. As shown in the example from Session 1 , you can selectively apply a configuration file to a process using the withName directive. Be specific with your selector Remember to make your selector specific to the process you are trying to customise. It can be helpful to use the same selectors that are already included in the configuration file. Sarek is a little different from other pipelines because it has multiple different config files that are used for different tools or groups of tools. This is stylistic and helps to keep the config files organised. For example, there are a number of options that are applied when calling variants with FREEBAYES and are all stored in a freebayes.config file together: freebayes.config process { withName: 'MERGE_FREEBAYES' { ext.prefix = { \"${meta.id}.freebayes\" } publishDir = [ mode: params.publish_dir_mode, path: { \"${params.outdir}/variant_calling/freebayes/${meta.id}/\" }, saveAs: { filename -> filename.equals('versions.yml') ? null : filename } ] } withName: 'FREEBAYES' { ext.args = '--min-alternate-fraction 0.1 --min-mapping-quality 1' //To make sure no naming conflicts ensure with module BCFTOOLS_SORT & the naming being correct in the output folder ext.prefix = { meta.num_intervals <= 1 ? \"${meta.id}\" : \"${meta.id}.${target_bed.simpleName}\" } ext.when = { params.tools && params.tools.split(',').contains('freebayes') } publishDir = [ enabled: false ] } withName: 'BCFTOOLS_SORT' { ext.prefix = { meta.num_intervals <= 1 ? meta.id + \".freebayes\" : vcf.name - \".vcf\" + \".sort\" } publishDir = [ mode: params.publish_dir_mode, path: { \"${params.outdir}/variant_calling/\" }, pattern: \"*vcf.gz\", saveAs: { meta.num_intervals > 1 ? null : \"freebayes/${meta.id}/${it}\" } ] } withName : 'TABIX_VC_FREEBAYES' { publishDir = [ mode: params.publish_dir_mode, path: { \"${params.outdir}/variant_calling/freebayes/${meta.id}/\" }, saveAs: { filename -> filename.equals('versions.yml') ? null : filename } ] } // PAIR_VARIANT_CALLING if (params.tools && params.tools.split(',').contains('freebayes')) { withName: '.*:BAM_VARIANT_CALLING_SOMATIC_ALL:BAM_VARIANT_CALLING_FREEBAYES:FREEBAYES' { ext.args = \"--pooled-continuous \\ --pooled-discrete \\ --genotype-qualities \\ --report-genotype-likelihood-max \\ --allele-balance-priors-off \\ --min-alternate-fraction 0.03 \\ --min-repeat-entropy 1 \\ --min-alternate-count 2 \" } } } Each of these can be modified independently of the others and be applied using a custom configuration file. Exercise Create a custom configuration file that will modify the --min-alternate-fraction parameter for FREEBAYES to 0.05 and apply it to the pipeline. Solution nextflow run nf-core/sarek --input samplesheet.csv -params-file my-params.json -profile singularity -r 3 .2.3 --tools \"freebayes,strelka\" -resume -c my-config.config my-config.config process { withName: 'FREEBAYES' { ext.args = '--min-alternate-fraction 0.05' } }","title":"Configuration files"},{"location":"setup/setup/","text":"Log into Nesi Jupyter \u00b6 During this workshop we will be running the material on the NeSI platform, using the Jupyter interface, however it is also possible to run this material locally on your own machine. One of the differences between running on NeSI or your own machine is that on NeSI we pre-install popular software and make it available to our users, whereas on your own machine you need to install the software yourself (e.g., using a package manager such as conda). Connect to Jupyter on NeSI - Make sure to spawn 4CPU , 8GB Jupyter sessions Connect to https://jupyter.nesi.org.nz Enter NeSI username, HPC password, and 6 digit second factor token (as set on MyNeSI ) Choose server options as below make sure to choose the correct project code nesi02659 , number of CPUs 4 , memory 8GB prior to pressing button. Start a terminal session from the JupyterLab launcher Loading required software \u00b6 This workshop will use a combination of \"environment modules\" and manually installed software. We will need to prepare our environment by running the following command to source init script which clear the environment, load required software and activate the pre-configured conda environment source /nesi/project/nesi02659/nextflow-workshop/init-nf-day1 Supplementary - How did we prepare the conda environment module purge module load Miniconda3 source $( conda info --base ) /etc/profile.d/conda.sh conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge export CONDA_ENVS_PATH = /nesi/project/nesi02659/.conda/envs #make sure the conda pkgs gets-redirected to nobackkup/scratch space #for more information, refer to https://support.nesi.org.nz/hc/en-gb/articles/360001580415-Miniconda3#prevent-conda-from-using-home-storage mkdir /nesi/nobackup/nesi02659/conda-pkgs/ $USER && conda config --add pkgs_dirs /nesi/nobackup/nesi02659/conda-pkgs/ $USER conda create --name nf-core python = 3 .11 nf-core nextflow --solver = libmamba -y More details about environment modules can be found on the NeSI support page .","title":"Log into Nesi Jupyter"},{"location":"setup/setup/#log-into-nesi-jupyter","text":"During this workshop we will be running the material on the NeSI platform, using the Jupyter interface, however it is also possible to run this material locally on your own machine. One of the differences between running on NeSI or your own machine is that on NeSI we pre-install popular software and make it available to our users, whereas on your own machine you need to install the software yourself (e.g., using a package manager such as conda). Connect to Jupyter on NeSI - Make sure to spawn 4CPU , 8GB Jupyter sessions Connect to https://jupyter.nesi.org.nz Enter NeSI username, HPC password, and 6 digit second factor token (as set on MyNeSI ) Choose server options as below make sure to choose the correct project code nesi02659 , number of CPUs 4 , memory 8GB prior to pressing button. Start a terminal session from the JupyterLab launcher","title":"Log into Nesi Jupyter"},{"location":"setup/setup/#loading-required-software","text":"This workshop will use a combination of \"environment modules\" and manually installed software. We will need to prepare our environment by running the following command to source init script which clear the environment, load required software and activate the pre-configured conda environment source /nesi/project/nesi02659/nextflow-workshop/init-nf-day1 Supplementary - How did we prepare the conda environment module purge module load Miniconda3 source $( conda info --base ) /etc/profile.d/conda.sh conda config --add channels defaults conda config --add channels bioconda conda config --add channels conda-forge export CONDA_ENVS_PATH = /nesi/project/nesi02659/.conda/envs #make sure the conda pkgs gets-redirected to nobackkup/scratch space #for more information, refer to https://support.nesi.org.nz/hc/en-gb/articles/360001580415-Miniconda3#prevent-conda-from-using-home-storage mkdir /nesi/nobackup/nesi02659/conda-pkgs/ $USER && conda config --add pkgs_dirs /nesi/nobackup/nesi02659/conda-pkgs/ $USER conda create --name nf-core python = 3 .11 nf-core nextflow --solver = libmamba -y More details about environment modules can be found on the NeSI support page .","title":"Loading required software"}]}